{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from pathlib import Path\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roc_data = {}\n",
    "all_prc_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decorator for running a function on multiple dataset splits\n",
    "def run_on_splits(func):\n",
    "    def _run_loop(model, splits, **kwargs):\n",
    "        results = {}\n",
    "        roc_data = {}\n",
    "        prc_data = {}\n",
    "        test_roc_data = {}\n",
    "        test_prc_data = {}\n",
    "        model_name = kwargs.get('model_name', 'model')\n",
    "        for split in splits:\n",
    "            X, y, nsplit = split\n",
    "            result, roc_info, prc_info = func(model, X, y, nsplit, **kwargs)\n",
    "            results[nsplit] = result\n",
    "            roc_data[nsplit] = roc_info\n",
    "            prc_data[nsplit] = prc_info\n",
    "            if nsplit == 'test':\n",
    "                test_roc_data = {model_name: roc_info}\n",
    "                test_prc_data = {model_name: prc_info}\n",
    "        return results, roc_data, prc_data, test_roc_data, test_prc_data\n",
    "    return _run_loop\n",
    "\n",
    "@run_on_splits\n",
    "def evaluate_classification(model, X, y, nsplit, model_name):\n",
    "    preds = model.predict(X)\n",
    "    pred_probs = model.predict_proba(X)[:, 1]\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    roc_auc = roc_auc_score(y, pred_probs)\n",
    "    fpr, tpr, _ = roc_curve(y, pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y, pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    report = classification_report(y, preds, output_dict=True)\n",
    "    print(f\"{model_name} - {nsplit} - Accuracy: {accuracy}, ROC_AUC: {roc_auc}, PRC_AUC: {prc_auc}\\n{report}\")\n",
    "    return (accuracy, report), (fpr, tpr, roc_auc), (precision, recall, prc_auc)\n",
    "\n",
    "def save_model_results(results, model_name, results_dir):\n",
    "    directory = results_dir\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, f'{model_name}_results.txt')\n",
    "    with open(filepath, 'w') as f:\n",
    "        for split, (accuracy, report) in results.items():\n",
    "            f.write(f\"{model_name} - {split} - Accuracy: {accuracy}\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            for key, value in report.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def plot_roc_curves(roc_data, model_name, results_dir, filename='roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (fpr, tpr, roc_auc) in roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} - {split} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prc_curves(prc_data, model_name, results_dir, filename='prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (precision, recall, prc_auc) in prc_data.items():\n",
    "        plt.plot(recall, precision, label=f'{model_name} - {split} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importances(model, model_name, feature_names, results_dir, filename='feature_importances.png'):\n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[-10:]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, (fpr, tpr, roc_auc) in all_roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Combined ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, prc_data in all_prc_data.items():\n",
    "        precision, recall, prc_auc = prc_data\n",
    "        plt.plot(recall, precision, label=f'{model_name} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Combined Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train_data_path = data_dir / \"train.csv\"\n",
    "    val_data_path = data_dir / \"val.csv\"\n",
    "    test_data_path = data_dir / \"test.csv\"\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    X_train = train_data.iloc[:, :-1].values\n",
    "    y_train = train_data.iloc[:, -1].values\n",
    "    X_val = val_data.iloc[:, :-1].values\n",
    "    y_val = val_data.iloc[:, -1].values\n",
    "    X_test = test_data.iloc[:, :-1].values\n",
    "    y_test = test_data.iloc[:, -1].values\n",
    "    feature_names = train_data.columns[:-1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, results_dir):\n",
    "    # Basic Random Forest model\n",
    "    basic_rfc = RandomForestClassifier(random_state=42)\n",
    "    basic_rfc.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_rfc, splits, model_name=\"Random_Forest_Basic\")\n",
    "    save_model_results(basic_results, \"Random_Forest_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Random_Forest_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Random_Forest_Basic\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(basic_rfc, \"Random_Forest_Basic\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Basic\"] = test_roc_data[\"Random_Forest_Basic\"]\n",
    "    all_prc_data[\"Random_Forest_Basic\"] = test_prc_data[\"Random_Forest_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned Random Forest model\n",
    "    rfc = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    cv_rfc = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, scoring='accuracy', n_iter=10, cv=3, random_state=42)\n",
    "    cv_rfc.fit(X_train, y_train)\n",
    "    best_params = cv_rfc.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_rfc.best_estimator_, splits, model_name=\"Random_Forest_Optimized\")\n",
    "    save_model_results(results, \"Random_Forest_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"Random_Forest_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"Random_Forest_Optimized\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(cv_rfc.best_estimator_, \"Random_Forest_Optimized\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Optimized\"] = test_roc_data[\"Random_Forest_Optimized\"]\n",
    "    all_prc_data[\"Random_Forest_Optimized\"] = test_prc_data[\"Random_Forest_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_clf_hyperparameters(clf, param_grid, X_train, y_train):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print(\"Best hyperparameters:\\n\", clf_grid.best_params_)\n",
    "    return clf_grid.best_estimator_\n",
    "\n",
    "def tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Basic XGBoost model\n",
    "    basic_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "    basic_model.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_model, splits, model_name=\"XGBoost_Basic\")\n",
    "    save_model_results(basic_results, \"XGBoost_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"XGBoost_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"XGBoost_Basic\", results_dir, filename='prc_curves.png')\n",
    "    \n",
    "    all_roc_data[\"XGBoost_Basic\"] = test_roc_data[\"XGBoost_Basic\"]\n",
    "    all_prc_data[\"XGBoost_Basic\"] = test_prc_data[\"XGBoost_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned XGBoost model\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': range(3, 10, 2),\n",
    "        'min_child_weight': range(1, 6, 2),\n",
    "        'learning_rate': [0.0001, 0.01, 0.1],\n",
    "        'n_estimators': [50, 200]\n",
    "    }\n",
    "    xgb_clf = xgb.XGBClassifier(random_state=0)\n",
    "    xgb_opt = tune_clf_hyperparameters(xgb_clf, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(xgb_opt, splits, model_name=\"XGBoost_Optimized\")\n",
    "    save_model_results(results, \"XGBoost_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"XGBoost_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"XGBoost_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"XGBoost_Optimized\"] = test_roc_data[\"XGBoost_Optimized\"]\n",
    "    all_prc_data[\"XGBoost_Optimized\"] = test_prc_data[\"XGBoost_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for SVM (with imputation)\n",
    "def preprocess_data_for_svm(X_train, X_val, X_test):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_val_imputed = imputer.transform(X_val)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    return X_train_imputed, X_val_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Train a basic SVM model\n",
    "    basic_svm = SVC(probability=True, random_state=42)\n",
    "    basic_svm.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_svm, splits, model_name=\"SVM_Basic\")\n",
    "    save_model_results(basic_results, \"SVM_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"SVM_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"SVM_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Basic\"] = test_roc_data[\"SVM_Basic\"]\n",
    "    all_prc_data[\"SVM_Basic\"] = test_prc_data[\"SVM_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "    cv_svm = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    cv_svm.fit(X_train, y_train)\n",
    "    best_params = cv_svm.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_svm.best_estimator_, splits, model_name=\"SVM_Optimized\")\n",
    "    save_model_results(results, \"SVM_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"SVM_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"SVM_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Optimized\"] = test_roc_data[\"SVM_Optimized\"]\n",
    "    all_prc_data[\"SVM_Optimized\"] = test_prc_data[\"SVM_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model, i.e., define the loss function and the optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print('Neural Network Test accuracy:', test_acc)\n",
    "\n",
    "    # Prepare results for consistency, this step is to compare with other models\n",
    "    test_predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    test_pred_probs = model.predict(X_test).flatten()\n",
    "    test_report = classification_report(y_test, test_predictions, output_dict=True)\n",
    "\n",
    "    # Calculate ROC and PRC data\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, test_pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "\n",
    "    results = {\n",
    "        'train': ('Not Evaluated', {}),\n",
    "        'val': ('Not Evaluated', {}),\n",
    "        'test': (test_acc, test_report)\n",
    "    }\n",
    "    save_model_results(results, \"Neural_Network\", results_dir)\n",
    "\n",
    "    # Store ROC and PRC data for the test set\n",
    "    test_roc_data = {\"Neural_Network\": (fpr, tpr, roc_auc)}\n",
    "    test_prc_data = {\"Neural_Network\": (precision, recall, prc_auc)}\n",
    "\n",
    "    all_roc_data[\"Neural_Network\"] = test_roc_data[\"Neural_Network\"]\n",
    "    all_prc_data[\"Neural_Network\"] = test_prc_data[\"Neural_Network\"]\n",
    "\n",
    "    # Plot ROC and PRC curves\n",
    "    plot_roc_curves(test_roc_data, \"Neural_Network\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(test_prc_data, \"Neural_Network\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    return results, test_roc_data, test_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a basic Logistic Regression model\n",
    "    basic_lr = LogisticRegression(random_state=42, max_iter=10000)\n",
    "    basic_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_lr, splits, model_name=\"Logistic_Regression_Basic\")\n",
    "    save_model_results(basic_results, \"Logistic_Regression_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Logistic_Regression_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Logistic_Regression_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Logistic_Regression_Basic\"] = test_roc_data[\"Logistic_Regression_Basic\"]\n",
    "    all_prc_data[\"Logistic_Regression_Basic\"] = test_prc_data[\"Logistic_Regression_Basic\"]\n",
    "\n",
    "    return basic_results, basic_roc_data, basic_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train an Elastic Net Logistic Regression model\n",
    "    elastic_net_lr = LogisticRegressionCV(cv=5, penalty='elasticnet', solver='saga', l1_ratios=[0.5], random_state=42, max_iter=10000)\n",
    "    elastic_net_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    enet_results, enet_roc_data, enet_prc_data, test_roc_data, test_prc_data = evaluate_classification(elastic_net_lr, splits, model_name=\"Elastic_Net_Logistic_Regression\")\n",
    "    save_model_results(enet_results, \"Elastic_Net_Logistic_Regression\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(enet_roc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(enet_prc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Elastic_Net_Logistic_Regression\"] = test_roc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "    all_prc_data[\"Elastic_Net_Logistic_Regression\"] = test_prc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "\n",
    "    return enet_results, enet_roc_data, enet_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts the majority class\n",
    "    majority_class_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "    majority_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    majority_results, majority_roc_data, majority_prc_data, test_roc_data, test_prc_data = evaluate_classification(majority_class_clf, splits, model_name=\"Majority_Class_Classifier\")\n",
    "    save_model_results(majority_results, \"Majority_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(majority_roc_data, \"Majority_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(majority_prc_data, \"Majority_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Majority_Class_Classifier\"] = test_roc_data[\"Majority_Class_Classifier\"]\n",
    "    all_prc_data[\"Majority_Class_Classifier\"] = test_prc_data[\"Majority_Class_Classifier\"]\n",
    "\n",
    "    return majority_results, majority_roc_data, majority_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts a random class\n",
    "    random_class_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "    random_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    random_results, random_roc_data, random_prc_data, test_roc_data, test_prc_data = evaluate_classification(random_class_clf, splits, model_name=\"Chance_Class_Classifier\")\n",
    "    save_model_results(random_results, \"Chance_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(random_roc_data, \"Chance_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(random_prc_data, \"Chance_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Chance_Class_Classifier\"] = test_roc_data[\"Chance_Class_Classifier\"]\n",
    "    all_prc_data[\"Chance_Class_Classifier\"] = test_prc_data[\"Chance_Class_Classifier\"]\n",
    "\n",
    "    return random_results, random_roc_data, random_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(data_dir, results_dir):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_data(data_dir)\n",
    "    \n",
    "    # create a separate directory for this dataset's results\n",
    "    dataset_name = data_dir.stem\n",
    "    dataset_results_dir = results_dir / dataset_name\n",
    "    os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Random Forest\n",
    "    results_rf, roc_data_rf, prc_data_rf = tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, dataset_results_dir)\n",
    "    \n",
    "    # XGBoost\n",
    "    results_xgb, roc_data_xgb, prc_data_xgb = tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # SVM\n",
    "    #results_svm, roc_data_svm, prc_data_svm = tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Neural Network\n",
    "    results_nn, roc_data_nn, prc_data_nn = tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    basic_results, basic_roc_data, basic_prc_data = evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Elastic Net Logistic Regression\n",
    "    enet_results, enet_roc_data, enet_prc_data = evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Majority Class Classifier\n",
    "    majority_results, majority_roc_data, majority_prc_data = evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Random Class Classifier\n",
    "    random_results, random_roc_data, random_prc_data = evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Plot combined PRC and ROC curves for all models\n",
    "    plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png')\n",
    "    plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1585\n",
      "1    1583\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Baseline Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    359\n",
      "1    321\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "Baseline Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    348\n",
      "1    332\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "-------------------------------------------------------------------\n",
      "Real Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1938\n",
      "1    1912\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "1    421\n",
      "0    405\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    433\n",
      "1    393\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# ------ OBS THIS IS JUST TO CHECK THAT THE DIFFERENT DATA SETS ARE COMPATIBLE (SO Y IS 0/1 INT AND NOT FLOATS)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd().parents[1]\n",
    "\n",
    "# Function to load datasets and ensure integer type for Lacto_Binary\n",
    "def load_and_verify(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Lacto_Binary'] = data['Lacto_Binary'].astype(int)  # Ensure integer type\n",
    "    return data\n",
    "\n",
    "# BASELINE DATA \n",
    "baseline_path_train = root / 'data' / 'baseline_demographic' / 'train.csv'\n",
    "baseline_path_val = root / 'data' / 'baseline_demographic' / 'val.csv'\n",
    "baseline_path_test = root / 'data' / 'baseline_demographic' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_baseline = load_and_verify(baseline_path_train)\n",
    "print(\"Baseline Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_baseline = load_and_verify(baseline_path_val)\n",
    "print(\"Baseline Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_baseline = load_and_verify(baseline_path_test)\n",
    "print(\"Baseline Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_baseline['Lacto_Binary'].unique())\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "# REAL DATA \n",
    "path_train = root / 'data' / 'reduced_0_1' / 'train.csv'\n",
    "path_val = root / 'data' / 'reduced_0_1' / 'val.csv'\n",
    "path_test = root / 'data' / 'reduced_0_1' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_real = load_and_verify(path_train)\n",
    "print(\"Real Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_real = load_and_verify(path_val)\n",
    "print(\"Real Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_real = load_and_verify(path_test)\n",
    "print(\"Real Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_real['Lacto_Binary'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Forest_Basic - train - Accuracy: 1.0, ROC_AUC: 1.0, PRC_AUC: 0.9999999999999999\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1912.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}}\n",
      "Random_Forest_Basic - val - Accuracy: 0.7493946731234867, ROC_AUC: 0.8431776194246503, PRC_AUC: 0.8626310306296758\n",
      "{'0.0': {'precision': 0.7133620689655172, 'recall': 0.817283950617284, 'f1-score': 0.761795166858458, 'support': 405.0}, '1.0': {'precision': 0.7955801104972375, 'recall': 0.684085510688836, 'f1-score': 0.735632183908046, 'support': 421.0}, 'accuracy': 0.7493946731234867, 'macro avg': {'precision': 0.7544710897313773, 'recall': 0.75068473065306, 'f1-score': 0.748713675383252, 'support': 826.0}, 'weighted avg': {'precision': 0.7552673903757525, 'recall': 0.7493946731234867, 'f1-score': 0.7484602808752576, 'support': 826.0}}\n",
      "Random_Forest_Basic - test - Accuracy: 0.7554479418886199, ROC_AUC: 0.8358572948069273, PRC_AUC: 0.8298706274955836\n",
      "{'0.0': {'precision': 0.766743648960739, 'recall': 0.766743648960739, 'f1-score': 0.766743648960739, 'support': 433.0}, '1.0': {'precision': 0.7430025445292621, 'recall': 0.7430025445292621, 'f1-score': 0.7430025445292621, 'support': 393.0}, 'accuracy': 0.7554479418886199, 'macro avg': {'precision': 0.7548730967450006, 'recall': 0.7548730967450006, 'f1-score': 0.7548730967450006, 'support': 826.0}, 'weighted avg': {'precision': 0.7554479418886199, 'recall': 0.7554479418886199, 'f1-score': 0.7554479418886199, 'support': 826.0}}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def main():\n",
    "    root = Path.cwd().parents[1]\n",
    "    data_dirs = [root / \"data\" / \"reduced_0_1\", root / \"data\" / \"reduced_0_1_PCA\", root / \"data\" / \"reduced_0_1_SVD\", root / \"data\" / \"baseline_demographic\"]\n",
    "    results_dir = root / \"results\" / \"model_reports\"\n",
    "    for data_dir in data_dirs:\n",
    "        run_all_models(data_dir, results_dir)\n",
    "\n",
    "# run the main function using \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
