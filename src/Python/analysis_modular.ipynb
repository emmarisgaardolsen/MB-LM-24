{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from pathlib import Path\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roc_data = {}\n",
    "all_prc_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decorator for running a function on multiple dataset splits\n",
    "def run_on_splits(func):\n",
    "    def _run_loop(model, splits, **kwargs):\n",
    "        results = {}\n",
    "        roc_data = {}\n",
    "        prc_data = {}\n",
    "        test_roc_data = {}\n",
    "        test_prc_data = {}\n",
    "        model_name = kwargs.get('model_name', 'model')\n",
    "        for split in splits:\n",
    "            X, y, nsplit = split\n",
    "            result, roc_info, prc_info = func(model, X, y, nsplit, **kwargs)\n",
    "            results[nsplit] = result\n",
    "            roc_data[nsplit] = roc_info\n",
    "            prc_data[nsplit] = prc_info\n",
    "            if nsplit == 'test':\n",
    "                test_roc_data = {model_name: roc_info}\n",
    "                test_prc_data = {model_name: prc_info}\n",
    "        return results, roc_data, prc_data, test_roc_data, test_prc_data\n",
    "    return _run_loop\n",
    "\n",
    "@run_on_splits\n",
    "def evaluate_classification(model, X, y, nsplit, model_name):\n",
    "    preds = model.predict(X)\n",
    "    pred_probs = model.predict_proba(X)[:, 1]\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    roc_auc = roc_auc_score(y, pred_probs)\n",
    "    fpr, tpr, _ = roc_curve(y, pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y, pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    report = classification_report(y, preds, output_dict=True)\n",
    "    print(f\"{model_name} - {nsplit} - Accuracy: {accuracy}, ROC_AUC: {roc_auc}, PRC_AUC: {prc_auc}\\n{report}\")\n",
    "    return (accuracy, report), (fpr, tpr, roc_auc), (precision, recall, prc_auc)\n",
    "\n",
    "def save_model_results(results, model_name, results_dir):\n",
    "    directory = results_dir\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, f'{model_name}_results.txt')\n",
    "    with open(filepath, 'w') as f:\n",
    "        for split, (accuracy, report) in results.items():\n",
    "            f.write(f\"{model_name} - {split} - Accuracy: {accuracy}\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            for key, value in report.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def plot_roc_curves(roc_data, model_name, results_dir, filename='roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (fpr, tpr, roc_auc) in roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} - {split} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prc_curves(prc_data, model_name, results_dir, filename='prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (precision, recall, prc_auc) in prc_data.items():\n",
    "        plt.plot(recall, precision, label=f'{model_name} - {split} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importances(model, model_name, feature_names, results_dir, filename='feature_importances.png'):\n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[-10:]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, (fpr, tpr, roc_auc) in all_roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Combined ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, prc_data in all_prc_data.items():\n",
    "        precision, recall, prc_auc = prc_data\n",
    "        plt.plot(recall, precision, label=f'{model_name} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Combined Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train_data_path = data_dir / \"train.csv\"\n",
    "    val_data_path = data_dir / \"val.csv\"\n",
    "    test_data_path = data_dir / \"test.csv\"\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    X_train = train_data.iloc[:, :-1].values\n",
    "    y_train = train_data.iloc[:, -1].values\n",
    "    X_val = val_data.iloc[:, :-1].values\n",
    "    y_val = val_data.iloc[:, -1].values\n",
    "    X_test = test_data.iloc[:, :-1].values\n",
    "    y_test = test_data.iloc[:, -1].values\n",
    "    feature_names = train_data.columns[:-1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, results_dir):\n",
    "    # Basic Random Forest model\n",
    "    basic_rfc = RandomForestClassifier(random_state=42)\n",
    "    basic_rfc.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_rfc, splits, model_name=\"Random_Forest_Basic\")\n",
    "    save_model_results(basic_results, \"Random_Forest_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Random_Forest_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Random_Forest_Basic\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(basic_rfc, \"Random_Forest_Basic\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Basic\"] = test_roc_data[\"Random_Forest_Basic\"]\n",
    "    all_prc_data[\"Random_Forest_Basic\"] = test_prc_data[\"Random_Forest_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned Random Forest model\n",
    "    rfc = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    cv_rfc = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, scoring='accuracy', n_iter=10, cv=3, random_state=42)\n",
    "    cv_rfc.fit(X_train, y_train)\n",
    "    best_params = cv_rfc.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_rfc.best_estimator_, splits, model_name=\"Random_Forest_Optimized\")\n",
    "    save_model_results(results, \"Random_Forest_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"Random_Forest_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"Random_Forest_Optimized\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(cv_rfc.best_estimator_, \"Random_Forest_Optimized\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Optimized\"] = test_roc_data[\"Random_Forest_Optimized\"]\n",
    "    all_prc_data[\"Random_Forest_Optimized\"] = test_prc_data[\"Random_Forest_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_clf_hyperparameters(clf, param_grid, X_train, y_train):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print(\"Best hyperparameters:\\n\", clf_grid.best_params_)\n",
    "    return clf_grid.best_estimator_\n",
    "\n",
    "def tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Basic XGBoost model\n",
    "    basic_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "    basic_model.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_model, splits, model_name=\"XGBoost_Basic\")\n",
    "    save_model_results(basic_results, \"XGBoost_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"XGBoost_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"XGBoost_Basic\", results_dir, filename='prc_curves.png')\n",
    "    \n",
    "    all_roc_data[\"XGBoost_Basic\"] = test_roc_data[\"XGBoost_Basic\"]\n",
    "    all_prc_data[\"XGBoost_Basic\"] = test_prc_data[\"XGBoost_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned XGBoost model\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': range(3, 10, 2),\n",
    "        'min_child_weight': range(1, 6, 2),\n",
    "        'learning_rate': [0.0001, 0.01, 0.1],\n",
    "        'n_estimators': [50, 200]\n",
    "    }\n",
    "    xgb_clf = xgb.XGBClassifier(random_state=0)\n",
    "    xgb_opt = tune_clf_hyperparameters(xgb_clf, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(xgb_opt, splits, model_name=\"XGBoost_Optimized\")\n",
    "    save_model_results(results, \"XGBoost_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"XGBoost_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"XGBoost_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"XGBoost_Optimized\"] = test_roc_data[\"XGBoost_Optimized\"]\n",
    "    all_prc_data[\"XGBoost_Optimized\"] = test_prc_data[\"XGBoost_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for SVM (with imputation)\n",
    "def preprocess_data_for_svm(X_train, X_val, X_test):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_val_imputed = imputer.transform(X_val)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    return X_train_imputed, X_val_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Train a basic SVM model\n",
    "    basic_svm = SVC(probability=True, random_state=42)\n",
    "    basic_svm.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_svm, splits, model_name=\"SVM_Basic\")\n",
    "    save_model_results(basic_results, \"SVM_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"SVM_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"SVM_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Basic\"] = test_roc_data[\"SVM_Basic\"]\n",
    "    all_prc_data[\"SVM_Basic\"] = test_prc_data[\"SVM_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "    cv_svm = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    cv_svm.fit(X_train, y_train)\n",
    "    best_params = cv_svm.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_svm.best_estimator_, splits, model_name=\"SVM_Optimized\")\n",
    "    save_model_results(results, \"SVM_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"SVM_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"SVM_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Optimized\"] = test_roc_data[\"SVM_Optimized\"]\n",
    "    all_prc_data[\"SVM_Optimized\"] = test_prc_data[\"SVM_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model, i.e., define the loss function and the optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print('Neural Network Test accuracy:', test_acc)\n",
    "\n",
    "    # Prepare results for consistency, this step is to compare with other models\n",
    "    test_predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    test_pred_probs = model.predict(X_test).flatten()\n",
    "    test_report = classification_report(y_test, test_predictions, output_dict=True)\n",
    "\n",
    "    # Calculate ROC and PRC data\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, test_pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "\n",
    "    results = {\n",
    "        'train': ('Not Evaluated', {}),\n",
    "        'val': ('Not Evaluated', {}),\n",
    "        'test': (test_acc, test_report)\n",
    "    }\n",
    "    save_model_results(results, \"Neural_Network\", results_dir)\n",
    "\n",
    "    # Store ROC and PRC data for the test set\n",
    "    test_roc_data = {\"Neural_Network\": (fpr, tpr, roc_auc)}\n",
    "    test_prc_data = {\"Neural_Network\": (precision, recall, prc_auc)}\n",
    "\n",
    "    all_roc_data[\"Neural_Network\"] = test_roc_data[\"Neural_Network\"]\n",
    "    all_prc_data[\"Neural_Network\"] = test_prc_data[\"Neural_Network\"]\n",
    "\n",
    "    # Plot ROC and PRC curves\n",
    "    plot_roc_curves(test_roc_data, \"Neural_Network\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(test_prc_data, \"Neural_Network\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    return results, test_roc_data, test_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a basic Logistic Regression model\n",
    "    basic_lr = LogisticRegression(random_state=42, max_iter=10000)\n",
    "    basic_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_lr, splits, model_name=\"Logistic_Regression_Basic\")\n",
    "    save_model_results(basic_results, \"Logistic_Regression_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Logistic_Regression_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Logistic_Regression_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Logistic_Regression_Basic\"] = test_roc_data[\"Logistic_Regression_Basic\"]\n",
    "    all_prc_data[\"Logistic_Regression_Basic\"] = test_prc_data[\"Logistic_Regression_Basic\"]\n",
    "\n",
    "    return basic_results, basic_roc_data, basic_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train an Elastic Net Logistic Regression model\n",
    "    elastic_net_lr = LogisticRegressionCV(cv=5, penalty='elasticnet', solver='saga', l1_ratios=[0.5], random_state=42, max_iter=10000)\n",
    "    elastic_net_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    enet_results, enet_roc_data, enet_prc_data, test_roc_data, test_prc_data = evaluate_classification(elastic_net_lr, splits, model_name=\"Elastic_Net_Logistic_Regression\")\n",
    "    save_model_results(enet_results, \"Elastic_Net_Logistic_Regression\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(enet_roc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(enet_prc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Elastic_Net_Logistic_Regression\"] = test_roc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "    all_prc_data[\"Elastic_Net_Logistic_Regression\"] = test_prc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "\n",
    "    return enet_results, enet_roc_data, enet_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts the majority class\n",
    "    majority_class_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "    majority_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    majority_results, majority_roc_data, majority_prc_data, test_roc_data, test_prc_data = evaluate_classification(majority_class_clf, splits, model_name=\"Majority_Class_Classifier\")\n",
    "    save_model_results(majority_results, \"Majority_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(majority_roc_data, \"Majority_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(majority_prc_data, \"Majority_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Majority_Class_Classifier\"] = test_roc_data[\"Majority_Class_Classifier\"]\n",
    "    all_prc_data[\"Majority_Class_Classifier\"] = test_prc_data[\"Majority_Class_Classifier\"]\n",
    "\n",
    "    return majority_results, majority_roc_data, majority_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts a random class\n",
    "    random_class_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "    random_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    random_results, random_roc_data, random_prc_data, test_roc_data, test_prc_data = evaluate_classification(random_class_clf, splits, model_name=\"Chance_Class_Classifier\")\n",
    "    save_model_results(random_results, \"Chance_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(random_roc_data, \"Chance_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(random_prc_data, \"Chance_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Chance_Class_Classifier\"] = test_roc_data[\"Chance_Class_Classifier\"]\n",
    "    all_prc_data[\"Chance_Class_Classifier\"] = test_prc_data[\"Chance_Class_Classifier\"]\n",
    "\n",
    "    return random_results, random_roc_data, random_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(data_dir, results_dir):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_data(data_dir)\n",
    "    \n",
    "    # create a separate directory for this dataset's results\n",
    "    dataset_name = data_dir.stem\n",
    "    dataset_results_dir = results_dir / dataset_name\n",
    "    os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Random Forest\n",
    "    results_rf, roc_data_rf, prc_data_rf = tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, dataset_results_dir)\n",
    "    \n",
    "    # XGBoost\n",
    "    results_xgb, roc_data_xgb, prc_data_xgb = tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # SVM\n",
    "    results_svm, roc_data_svm, prc_data_svm = tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Neural Network\n",
    "    results_nn, roc_data_nn, prc_data_nn = tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    basic_results, basic_roc_data, basic_prc_data = evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Elastic Net Logistic Regression\n",
    "    enet_results, enet_roc_data, enet_prc_data = evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Majority Class Classifier\n",
    "    majority_results, majority_roc_data, majority_prc_data = evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Random Class Classifier\n",
    "    random_results, random_roc_data, random_prc_data = evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Plot combined PRC and ROC curves for all models\n",
    "    plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png')\n",
    "    plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1585\n",
      "1    1583\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Baseline Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    359\n",
      "1    321\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "Baseline Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    348\n",
      "1    332\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "-------------------------------------------------------------------\n",
      "Real Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1938\n",
      "1    1912\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "1    421\n",
      "0    405\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    433\n",
      "1    393\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# ------ OBS THIS IS JUST TO CHECK THAT THE DIFFERENT DATA SETS ARE COMPATIBLE (SO Y IS 0/1 INT AND NOT FLOATS)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd().parents[1]\n",
    "\n",
    "# Function to load datasets and ensure integer type for Lacto_Binary\n",
    "def load_and_verify(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Lacto_Binary'] = data['Lacto_Binary'].astype(int)  # Ensure integer type\n",
    "    return data\n",
    "\n",
    "# BASELINE DATA \n",
    "baseline_path_train = root / 'data' / 'baseline_demographic' / 'train.csv'\n",
    "baseline_path_val = root / 'data' / 'baseline_demographic' / 'val.csv'\n",
    "baseline_path_test = root / 'data' / 'baseline_demographic' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_baseline = load_and_verify(baseline_path_train)\n",
    "print(\"Baseline Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_baseline = load_and_verify(baseline_path_val)\n",
    "print(\"Baseline Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_baseline = load_and_verify(baseline_path_test)\n",
    "print(\"Baseline Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_baseline['Lacto_Binary'].unique())\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "# REAL DATA \n",
    "path_train = root / 'data' / 'reduced_0_1' / 'train.csv'\n",
    "path_val = root / 'data' / 'reduced_0_1' / 'val.csv'\n",
    "path_test = root / 'data' / 'reduced_0_1' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_real = load_and_verify(path_train)\n",
    "print(\"Real Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_real = load_and_verify(path_val)\n",
    "print(\"Real Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_real = load_and_verify(path_test)\n",
    "print(\"Real Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_real['Lacto_Binary'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Forest_Basic - train - Accuracy: 1.0, ROC_AUC: 1.0, PRC_AUC: 0.9999999999999999\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1912.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}}\n",
      "Random_Forest_Basic - val - Accuracy: 0.8849878934624698, ROC_AUC: 0.9500777103310754, PRC_AUC: 0.9577768425177831\n",
      "{'0.0': {'precision': 0.8571428571428571, 'recall': 0.9185185185185185, 'f1-score': 0.8867699642431466, 'support': 405.0}, '1.0': {'precision': 0.9158163265306123, 'recall': 0.8527315914489311, 'f1-score': 0.8831488314883149, 'support': 421.0}, 'accuracy': 0.8849878934624698, 'macro avg': {'precision': 0.8864795918367347, 'recall': 0.8856250549837248, 'f1-score': 0.8849593978657307, 'support': 826.0}, 'weighted avg': {'precision': 0.8870478578840736, 'recall': 0.8849878934624698, 'f1-score': 0.884924326362052, 'support': 826.0}}\n",
      "Random_Forest_Basic - test - Accuracy: 0.9043583535108959, ROC_AUC: 0.9624549712344787, PRC_AUC: 0.9661355721172905\n",
      "{'0.0': {'precision': 0.8915929203539823, 'recall': 0.930715935334873, 'f1-score': 0.9107344632768362, 'support': 433.0}, '1.0': {'precision': 0.9197860962566845, 'recall': 0.8753180661577609, 'f1-score': 0.8970013037809648, 'support': 393.0}, 'accuracy': 0.9043583535108959, 'macro avg': {'precision': 0.9056895083053333, 'recall': 0.903017000746317, 'f1-score': 0.9038678835289005, 'support': 826.0}, 'weighted avg': {'precision': 0.9050068648210066, 'recall': 0.9043583535108959, 'f1-score': 0.9042004055505922, 'support': 826.0}}\n",
      "Best parameters: {'n_estimators': 10, 'min_samples_split': 2, 'max_features': None, 'max_depth': None}\n",
      "Random_Forest_Optimized - train - Accuracy: 0.9971428571428571, ROC_AUC: 0.9999828631078064, PRC_AUC: 0.9999826622414777\n",
      "{'0.0': {'precision': 0.9958826556870818, 'recall': 0.9984520123839009, 'f1-score': 0.9971656789487245, 'support': 1938.0}, '1.0': {'precision': 0.9984268484530676, 'recall': 0.99581589958159, 'f1-score': 0.9971196648337262, 'support': 1912.0}, 'accuracy': 0.9971428571428571, 'macro avg': {'precision': 0.9971547520700748, 'recall': 0.9971339559827455, 'f1-score': 0.9971426718912253, 'support': 3850.0}, 'weighted avg': {'precision': 0.9971461612893064, 'recall': 0.9971428571428571, 'f1-score': 0.9971428272635617, 'support': 3850.0}}\n",
      "Random_Forest_Optimized - val - Accuracy: 0.9891041162227603, ROC_AUC: 0.9972082930119353, PRC_AUC: 0.998443605264625\n",
      "{'0.0': {'precision': 0.9876847290640394, 'recall': 0.9901234567901235, 'f1-score': 0.9889025893958077, 'support': 405.0}, '1.0': {'precision': 0.9904761904761905, 'recall': 0.9881235154394299, 'f1-score': 0.9892984542211652, 'support': 421.0}, 'accuracy': 0.9891041162227603, 'macro avg': {'precision': 0.9890804597701149, 'recall': 0.9891234861147766, 'f1-score': 0.9891005218084865, 'support': 826.0}, 'weighted avg': {'precision': 0.9891074957159954, 'recall': 0.9891041162227603, 'f1-score': 0.9891043558503786, 'support': 826.0}}\n",
      "Random_Forest_Optimized - test - Accuracy: 0.9891041162227603, ROC_AUC: 0.9969735968360867, PRC_AUC: 0.9981603879983652\n",
      "{'0.0': {'precision': 0.9840182648401826, 'recall': 0.9953810623556582, 'f1-score': 0.9896670493685419, 'support': 433.0}, '1.0': {'precision': 0.9948453608247423, 'recall': 0.9821882951653944, 'f1-score': 0.9884763124199744, 'support': 393.0}, 'accuracy': 0.9891041162227603, 'macro avg': {'precision': 0.9894318128324624, 'recall': 0.9887846787605263, 'f1-score': 0.9890716808942581, 'support': 826.0}, 'weighted avg': {'precision': 0.9891696555447006, 'recall': 0.9891041162227603, 'f1-score': 0.9891005122973711, 'support': 826.0}}\n",
      "XGBoost_Basic - train - Accuracy: 1.0, ROC_AUC: 1.0, PRC_AUC: 1.0\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1912.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}}\n",
      "XGBoost_Basic - val - Accuracy: 0.9745762711864406, ROC_AUC: 0.9960880912583209, PRC_AUC: 0.9969763133943088\n",
      "{'0.0': {'precision': 0.9528301886792453, 'recall': 0.9975308641975309, 'f1-score': 0.9746682750301568, 'support': 405.0}, '1.0': {'precision': 0.9975124378109452, 'recall': 0.9524940617577197, 'f1-score': 0.9744835965978129, 'support': 421.0}, 'accuracy': 0.9745762711864406, 'macro avg': {'precision': 0.9751713132450952, 'recall': 0.9750124629776253, 'f1-score': 0.9745759358139849, 'support': 826.0}, 'weighted avg': {'precision': 0.975604071105935, 'recall': 0.9745762711864406, 'f1-score': 0.9745741471608871, 'support': 826.0}}\n",
      "XGBoost_Basic - test - Accuracy: 0.9782082324455206, ROC_AUC: 0.9942997843320465, PRC_AUC: 0.9958143186095364\n",
      "{'0.0': {'precision': 0.9683972911963883, 'recall': 0.9907621247113164, 'f1-score': 0.9794520547945206, 'support': 433.0}, '1.0': {'precision': 0.9895561357702349, 'recall': 0.9643765903307888, 'f1-score': 0.9768041237113402, 'support': 393.0}, 'accuracy': 0.9782082324455206, 'macro avg': {'precision': 0.9789767134833116, 'recall': 0.9775693575210527, 'f1-score': 0.9781280892529304, 'support': 826.0}, 'weighted avg': {'precision': 0.9784643927914509, 'recall': 0.9782082324455206, 'f1-score': 0.9781922038070026, 'support': 826.0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90303.05s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.10s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.10s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.10s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.16s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.19s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.22s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "90303.23s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      " {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "XGBoost_Optimized - train - Accuracy: 0.9976623376623377, ROC_AUC: 0.9999989205107279, PRC_AUC: 0.9999989078306699\n",
      "{'0.0': {'precision': 0.9953775038520801, 'recall': 1.0, 'f1-score': 0.9976833976833976, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 0.9952928870292888, 'f1-score': 0.9976408912188729, 'support': 1912.0}, 'accuracy': 0.9976623376623377, 'macro avg': {'precision': 0.99768875192604, 'recall': 0.9976464435146444, 'f1-score': 0.9976621444511353, 'support': 3850.0}, 'weighted avg': {'precision': 0.9976731434974887, 'recall': 0.9976623376623377, 'f1-score': 0.997662287979457, 'support': 3850.0}}\n",
      "XGBoost_Optimized - val - Accuracy: 0.9806295399515739, ROC_AUC: 0.9950910530482978, PRC_AUC: 0.9966310161195652\n",
      "{'0.0': {'precision': 0.9619952494061758, 'recall': 1.0, 'f1-score': 0.9806295399515739, 'support': 405.0}, '1.0': {'precision': 1.0, 'recall': 0.9619952494061758, 'f1-score': 0.9806295399515739, 'support': 421.0}, 'accuracy': 0.9806295399515739, 'macro avg': {'precision': 0.9809976247030878, 'recall': 0.9809976247030878, 'f1-score': 0.9806295399515739, 'support': 826.0}, 'weighted avg': {'precision': 0.981365709454602, 'recall': 0.9806295399515739, 'f1-score': 0.9806295399515739, 'support': 826.0}}\n",
      "XGBoost_Optimized - test - Accuracy: 0.9842615012106537, ROC_AUC: 0.9967267833741751, PRC_AUC: 0.9973828171756501\n",
      "{'0.0': {'precision': 0.9772727272727273, 'recall': 0.9930715935334873, 'f1-score': 0.9851088201603666, 'support': 433.0}, '1.0': {'precision': 0.9922279792746114, 'recall': 0.9745547073791349, 'f1-score': 0.9833119383825417, 'support': 393.0}, 'accuracy': 0.9842615012106537, 'macro avg': {'precision': 0.9847503532736693, 'recall': 0.983813150456311, 'f1-score': 0.9842103792714542, 'support': 826.0}, 'weighted avg': {'precision': 0.9843882406343986, 'recall': 0.9842615012106537, 'f1-score': 0.9842538873048154, 'support': 826.0}}\n",
      "SVM_Basic - train - Accuracy: 0.5366233766233767, ROC_AUC: 0.5624031158378349, PRC_AUC: 0.5631167234212746\n",
      "{'0.0': {'precision': 0.5442020665901263, 'recall': 0.4891640866873065, 'f1-score': 0.5152173913043478, 'support': 1938.0}, '1.0': {'precision': 0.530360531309298, 'recall': 0.5847280334728033, 'f1-score': 0.5562189054726369, 'support': 1912.0}, 'accuracy': 0.5366233766233767, 'macro avg': {'precision': 0.5372812989497121, 'recall': 0.5369460600800549, 'f1-score': 0.5357181483884923, 'support': 3850.0}, 'weighted avg': {'precision': 0.5373280366013098, 'recall': 0.5366233766233767, 'f1-score': 0.5355797017172748, 'support': 3850.0}}\n",
      "SVM_Basic - val - Accuracy: 0.5230024213075061, ROC_AUC: 0.550458930823143, PRC_AUC: 0.5644180631554014\n",
      "{'0.0': {'precision': 0.5139240506329114, 'recall': 0.5012345679012346, 'f1-score': 0.5075, 'support': 405.0}, '1.0': {'precision': 0.531322505800464, 'recall': 0.5439429928741093, 'f1-score': 0.5375586854460094, 'support': 421.0}, 'accuracy': 0.5230024213075061, 'macro avg': {'precision': 0.5226232782166877, 'recall': 0.5225887803876719, 'f1-score': 0.5225293427230047, 'support': 826.0}, 'weighted avg': {'precision': 0.5227917862570515, 'recall': 0.5230024213075061, 'f1-score': 0.5228204680057748, 'support': 826.0}}\n",
      "SVM_Basic - test - Accuracy: 0.5012106537530266, ROC_AUC: 0.535200300877363, PRC_AUC: 0.5296744005651987\n",
      "{'0.0': {'precision': 0.5300859598853869, 'recall': 0.42725173210161665, 'f1-score': 0.4731457800511509, 'support': 433.0}, '1.0': {'precision': 0.480083857442348, 'recall': 0.5826972010178118, 'f1-score': 0.5264367816091954, 'support': 393.0}, 'accuracy': 0.5012106537530266, 'macro avg': {'precision': 0.5050849086638675, 'recall': 0.5049744665597142, 'f1-score': 0.4997912808301731, 'support': 826.0}, 'weighted avg': {'precision': 0.5062956133235051, 'recall': 0.5012106537530266, 'f1-score': 0.49850094180939725, 'support': 826.0}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[682], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m         run_all_models(data_dir, results_dir)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# run the main function using \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[682], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m results_dir \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_reports\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_dir \u001b[38;5;129;01min\u001b[39;00m data_dirs:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mrun_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[680], line 16\u001b[0m, in \u001b[0;36mrun_all_models\u001b[0;34m(data_dir, results_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m results_xgb, roc_data_xgb, prc_data_xgb \u001b[38;5;241m=\u001b[39m tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# SVM\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m results_svm, roc_data_svm, prc_data_svm \u001b[38;5;241m=\u001b[39m \u001b[43mtune_and_evaluate_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_results_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Neural Network\u001b[39;00m\n\u001b[1;32m     19\u001b[0m results_nn, roc_data_nn, prc_data_nn \u001b[38;5;241m=\u001b[39m tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
      "Cell \u001b[0;32mIn[674], line 24\u001b[0m, in \u001b[0;36mtune_and_evaluate_svm\u001b[0;34m(X_train, y_train, X_val, y_val, X_test, y_test, results_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.001\u001b[39m],\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m cv_svm \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39msvm, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mcv_svm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m best_params \u001b[38;5;241m=\u001b[39m cv_svm\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/microbiome-ML/venv/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def main():\n",
    "    root = Path.cwd().parents[1]\n",
    "    data_dirs = [root / \"data\" / \"reduced_0_1\", root / \"data\" / \"reduced_0_1_PCA\", root / \"data\" / \"reduced_0_1_SVD\", root / \"data\" / \"baseline_demographic\"]\n",
    "    results_dir = root / \"results\" / \"model_reports\"\n",
    "    for data_dir in data_dirs:\n",
    "        run_all_models(data_dir, results_dir)\n",
    "\n",
    "# run the main function using \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
