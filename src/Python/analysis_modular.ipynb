{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 10:53:46.041545: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 10:53:47.608081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from pathlib import Path\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roc_data = {}\n",
    "all_prc_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decorator for running a function on multiple dataset splits\n",
    "def run_on_splits(func):\n",
    "    def _run_loop(model, splits, **kwargs):\n",
    "        results = {}\n",
    "        roc_data = {}\n",
    "        prc_data = {}\n",
    "        test_roc_data = {}\n",
    "        test_prc_data = {}\n",
    "        model_name = kwargs.get('model_name', 'model')\n",
    "        for split in splits:\n",
    "            X, y, nsplit = split\n",
    "            result, roc_info, prc_info = func(model, X, y, nsplit, **kwargs)\n",
    "            results[nsplit] = result\n",
    "            roc_data[nsplit] = roc_info\n",
    "            prc_data[nsplit] = prc_info\n",
    "            if nsplit == 'test':\n",
    "                test_roc_data = {model_name: roc_info}\n",
    "                test_prc_data = {model_name: prc_info}\n",
    "        return results, roc_data, prc_data, test_roc_data, test_prc_data\n",
    "    return _run_loop\n",
    "\n",
    "@run_on_splits\n",
    "def evaluate_classification(model, X, y, nsplit, model_name):\n",
    "    preds = model.predict(X)\n",
    "    pred_probs = model.predict_proba(X)[:, 1]\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    roc_auc = roc_auc_score(y, pred_probs)\n",
    "    fpr, tpr, _ = roc_curve(y, pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y, pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "    report = classification_report(y, preds, output_dict=True)\n",
    "    print(f\"{model_name} - {nsplit} - Accuracy: {accuracy}, ROC_AUC: {roc_auc}, PRC_AUC: {prc_auc}\\n{report}\")\n",
    "    return (accuracy, report), (fpr, tpr, roc_auc), (precision, recall, prc_auc)\n",
    "\n",
    "def save_model_results(results, model_name, results_dir):\n",
    "    directory = results_dir\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, f'{model_name}_results.txt')\n",
    "    with open(filepath, 'w') as f:\n",
    "        for split, (accuracy, report) in results.items():\n",
    "            f.write(f\"{model_name} - {split} - Accuracy: {accuracy}\\n\")\n",
    "            f.write(\"Classification Report:\\n\")\n",
    "            for key, value in report.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def plot_roc_curves(roc_data, model_name, results_dir, filename='roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (fpr, tpr, roc_auc) in roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} - {split} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_prc_curves(prc_data, model_name, results_dir, filename='prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for split, (precision, recall, prc_auc) in prc_data.items():\n",
    "        plt.plot(recall, precision, label=f'{model_name} - {split} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importances(model, model_name, feature_names, results_dir, filename='feature_importances.png'):\n",
    "    feature_importances = model.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[-10:]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), feature_importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    full_path = os.path.join(results_dir, f'{model_name}_{filename}')\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, (fpr, tpr, roc_auc) in all_roc_data.items():\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (ROC AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Combined ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png'):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model_name, prc_data in all_prc_data.items():\n",
    "        precision, recall, prc_auc = prc_data\n",
    "        plt.plot(recall, precision, label=f'{model_name} (PRC AUC = {prc_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Combined Precision-Recall Curves')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    full_path = os.path.join(results_dir, filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "\n",
    "def load_data(data_dir):\n",
    "    train_data_path = data_dir / \"train.csv\"\n",
    "    val_data_path = data_dir / \"val.csv\"\n",
    "    test_data_path = data_dir / \"test.csv\"\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    val_data = pd.read_csv(val_data_path)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    X_train = train_data.iloc[:, :-1].values\n",
    "    y_train = train_data.iloc[:, -1].values\n",
    "    X_val = val_data.iloc[:, :-1].values\n",
    "    y_val = val_data.iloc[:, -1].values\n",
    "    X_test = test_data.iloc[:, :-1].values\n",
    "    y_test = test_data.iloc[:, -1].values\n",
    "    feature_names = train_data.columns[:-1]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, results_dir):\n",
    "    # Basic Random Forest model\n",
    "    basic_rfc = RandomForestClassifier(random_state=42)\n",
    "    basic_rfc.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_rfc, splits, model_name=\"Random_Forest_Basic\")\n",
    "    save_model_results(basic_results, \"Random_Forest_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Random_Forest_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Random_Forest_Basic\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(basic_rfc, \"Random_Forest_Basic\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Basic\"] = test_roc_data[\"Random_Forest_Basic\"]\n",
    "    all_prc_data[\"Random_Forest_Basic\"] = test_prc_data[\"Random_Forest_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned Random Forest model\n",
    "    rfc = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    cv_rfc = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, scoring='accuracy', n_iter=10, cv=3, random_state=42)\n",
    "    cv_rfc.fit(X_train, y_train)\n",
    "    best_params = cv_rfc.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_rfc.best_estimator_, splits, model_name=\"Random_Forest_Optimized\")\n",
    "    save_model_results(results, \"Random_Forest_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"Random_Forest_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"Random_Forest_Optimized\", results_dir, filename='prc_curves.png')\n",
    "    plot_feature_importances(cv_rfc.best_estimator_, \"Random_Forest_Optimized\", feature_names, results_dir, filename='feature_importances.png')\n",
    "\n",
    "    all_roc_data[\"Random_Forest_Optimized\"] = test_roc_data[\"Random_Forest_Optimized\"]\n",
    "    all_prc_data[\"Random_Forest_Optimized\"] = test_prc_data[\"Random_Forest_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_clf_hyperparameters(clf, param_grid, X_train, y_train):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print(\"Best hyperparameters:\\n\", clf_grid.best_params_)\n",
    "    return clf_grid.best_estimator_\n",
    "\n",
    "def tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Basic XGBoost model\n",
    "    basic_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "    basic_model.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_model, splits, model_name=\"XGBoost_Basic\")\n",
    "    save_model_results(basic_results, \"XGBoost_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"XGBoost_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"XGBoost_Basic\", results_dir, filename='prc_curves.png')\n",
    "    \n",
    "    all_roc_data[\"XGBoost_Basic\"] = test_roc_data[\"XGBoost_Basic\"]\n",
    "    all_prc_data[\"XGBoost_Basic\"] = test_prc_data[\"XGBoost_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned XGBoost model\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': range(3, 10, 2),\n",
    "        'min_child_weight': range(1, 6, 2),\n",
    "        'learning_rate': [0.0001, 0.01, 0.1],\n",
    "        'n_estimators': [50, 200]\n",
    "    }\n",
    "    xgb_clf = xgb.XGBClassifier(random_state=0)\n",
    "    xgb_opt = tune_clf_hyperparameters(xgb_clf, xgb_param_grid, X_train, y_train)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(xgb_opt, splits, model_name=\"XGBoost_Optimized\")\n",
    "    save_model_results(results, \"XGBoost_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"XGBoost_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"XGBoost_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"XGBoost_Optimized\"] = test_roc_data[\"XGBoost_Optimized\"]\n",
    "    all_prc_data[\"XGBoost_Optimized\"] = test_prc_data[\"XGBoost_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess data for SVM (with imputation)\n",
    "def preprocess_data_for_svm(X_train, X_val, X_test):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train)\n",
    "    X_val_imputed = imputer.transform(X_val)\n",
    "    X_test_imputed = imputer.transform(X_test)\n",
    "    return X_train_imputed, X_val_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Train a basic SVM model\n",
    "    basic_svm = SVC(probability=True, random_state=42)\n",
    "    basic_svm.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_svm, splits, model_name=\"SVM_Basic\")\n",
    "    save_model_results(basic_results, \"SVM_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"SVM_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"SVM_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Basic\"] = test_roc_data[\"SVM_Basic\"]\n",
    "    all_prc_data[\"SVM_Basic\"] = test_prc_data[\"SVM_Basic\"]\n",
    "\n",
    "    # Hyperparameter-tuned SVM model\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "    }\n",
    "    cv_svm = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
    "    cv_svm.fit(X_train, y_train)\n",
    "    best_params = cv_svm.best_params_\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    results, roc_data, prc_data, test_roc_data, test_prc_data = evaluate_classification(cv_svm.best_estimator_, splits, model_name=\"SVM_Optimized\")\n",
    "    save_model_results(results, \"SVM_Optimized\", results_dir)\n",
    "\n",
    "    plot_roc_curves(roc_data, \"SVM_Optimized\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(prc_data, \"SVM_Optimized\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"SVM_Optimized\"] = test_roc_data[\"SVM_Optimized\"]\n",
    "    all_prc_data[\"SVM_Optimized\"] = test_prc_data[\"SVM_Optimized\"]\n",
    "\n",
    "    return results, roc_data, prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model, i.e., define the loss function and the optimizer\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print('Neural Network Test accuracy:', test_acc)\n",
    "\n",
    "    # Prepare results for consistency, this step is to compare with other models\n",
    "    test_predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    test_pred_probs = model.predict(X_test).flatten()\n",
    "    test_report = classification_report(y_test, test_predictions, output_dict=True)\n",
    "\n",
    "    # Calculate ROC and PRC data\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_pred_probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_pred_probs)\n",
    "    roc_auc = roc_auc_score(y_test, test_pred_probs)\n",
    "    prc_auc = auc(recall, precision)\n",
    "\n",
    "    results = {\n",
    "        'train': ('Not Evaluated', {}),\n",
    "        'val': ('Not Evaluated', {}),\n",
    "        'test': (test_acc, test_report)\n",
    "    }\n",
    "    save_model_results(results, \"Neural_Network\", results_dir)\n",
    "\n",
    "    # Store ROC and PRC data for the test set\n",
    "    test_roc_data = {\"Neural_Network\": (fpr, tpr, roc_auc)}\n",
    "    test_prc_data = {\"Neural_Network\": (precision, recall, prc_auc)}\n",
    "\n",
    "    all_roc_data[\"Neural_Network\"] = test_roc_data[\"Neural_Network\"]\n",
    "    all_prc_data[\"Neural_Network\"] = test_prc_data[\"Neural_Network\"]\n",
    "\n",
    "    # Plot ROC and PRC curves\n",
    "    plot_roc_curves(test_roc_data, \"Neural_Network\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(test_prc_data, \"Neural_Network\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    return results, test_roc_data, test_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a basic Logistic Regression model\n",
    "    basic_lr = LogisticRegression(random_state=42, max_iter=10000)\n",
    "    basic_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    basic_results, basic_roc_data, basic_prc_data, test_roc_data, test_prc_data = evaluate_classification(basic_lr, splits, model_name=\"Logistic_Regression_Basic\")\n",
    "    save_model_results(basic_results, \"Logistic_Regression_Basic\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(basic_roc_data, \"Logistic_Regression_Basic\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(basic_prc_data, \"Logistic_Regression_Basic\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Logistic_Regression_Basic\"] = test_roc_data[\"Logistic_Regression_Basic\"]\n",
    "    all_prc_data[\"Logistic_Regression_Basic\"] = test_prc_data[\"Logistic_Regression_Basic\"]\n",
    "\n",
    "    return basic_results, basic_roc_data, basic_prc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train an Elastic Net Logistic Regression model\n",
    "    elastic_net_lr = LogisticRegressionCV(cv=5, penalty='elasticnet', solver='saga', l1_ratios=[0.5], random_state=42, max_iter=10000)\n",
    "    elastic_net_lr.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    enet_results, enet_roc_data, enet_prc_data, test_roc_data, test_prc_data = evaluate_classification(elastic_net_lr, splits, model_name=\"Elastic_Net_Logistic_Regression\")\n",
    "    save_model_results(enet_results, \"Elastic_Net_Logistic_Regression\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(enet_roc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(enet_prc_data, \"Elastic_Net_Logistic_Regression\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Elastic_Net_Logistic_Regression\"] = test_roc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "    all_prc_data[\"Elastic_Net_Logistic_Regression\"] = test_prc_data[\"Elastic_Net_Logistic_Regression\"]\n",
    "\n",
    "    return enet_results, enet_roc_data, enet_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts the majority class\n",
    "    majority_class_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "    majority_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    majority_results, majority_roc_data, majority_prc_data, test_roc_data, test_prc_data = evaluate_classification(majority_class_clf, splits, model_name=\"Majority_Class_Classifier\")\n",
    "    save_model_results(majority_results, \"Majority_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(majority_roc_data, \"Majority_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(majority_prc_data, \"Majority_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Majority_Class_Classifier\"] = test_roc_data[\"Majority_Class_Classifier\"]\n",
    "    all_prc_data[\"Majority_Class_Classifier\"] = test_prc_data[\"Majority_Class_Classifier\"]\n",
    "\n",
    "    return majority_results, majority_roc_data, majority_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n",
    "    # train a dummy classifier that predicts a random class\n",
    "    random_class_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "    random_class_clf.fit(X_train, y_train)\n",
    "    \n",
    "    splits = [(X_train, y_train, 'train'), (X_val, y_val, 'val'), (X_test, y_test, 'test')]\n",
    "    random_results, random_roc_data, random_prc_data, test_roc_data, test_prc_data = evaluate_classification(random_class_clf, splits, model_name=\"Chance_Class_Classifier\")\n",
    "    save_model_results(random_results, \"Chance_Class_Classifier\", results_dir)\n",
    "    \n",
    "    plot_roc_curves(random_roc_data, \"Chance_Class_Classifier\", results_dir, filename='roc_curves.png')\n",
    "    plot_prc_curves(random_prc_data, \"Chance_Class_Classifier\", results_dir, filename='prc_curves.png')\n",
    "\n",
    "    all_roc_data[\"Chance_Class_Classifier\"] = test_roc_data[\"Chance_Class_Classifier\"]\n",
    "    all_prc_data[\"Chance_Class_Classifier\"] = test_prc_data[\"Chance_Class_Classifier\"]\n",
    "\n",
    "    return random_results, random_roc_data, random_prc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(data_dir, results_dir):\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_data(data_dir)\n",
    "    \n",
    "    # create a separate directory for this dataset's results\n",
    "    dataset_name = data_dir.stem\n",
    "    dataset_results_dir = results_dir / dataset_name\n",
    "    os.makedirs(dataset_results_dir, exist_ok=True)\n",
    "    \n",
    "    # Random Forest\n",
    "    results_rf, roc_data_rf, prc_data_rf = tune_and_evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, feature_names, dataset_results_dir)\n",
    "    \n",
    "    # XGBoost\n",
    "    results_xgb, roc_data_xgb, prc_data_xgb = tune_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # SVM\n",
    "    #results_svm, roc_data_svm, prc_data_svm = tune_and_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Neural Network\n",
    "    results_nn, roc_data_nn, prc_data_nn = tune_and_evaluate_neural_network(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    basic_results, basic_roc_data, basic_prc_data = evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Elastic Net Logistic Regression\n",
    "    enet_results, enet_roc_data, enet_prc_data = evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Majority Class Classifier\n",
    "    majority_results, majority_roc_data, majority_prc_data = evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Random Class Classifier\n",
    "    random_results, random_roc_data, random_prc_data = evaluate_Chance_Class_Classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
    "    \n",
    "    # Plot combined PRC and ROC curves for all models\n",
    "    plot_combined_prc_curves(all_prc_data, results_dir, filename='all_prc_curves.png')\n",
    "    plot_combined_roc_curves(all_roc_data, results_dir, filename='all_roc_curves.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1585\n",
      "1    1583\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Baseline Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    359\n",
      "1    321\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "Baseline Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    348\n",
      "1    332\n",
      "Name: count, dtype: int64\n",
      "Unique values: [0 1]\n",
      "-------------------------------------------------------------------\n",
      "Real Training Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    1938\n",
      "1    1912\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Validation Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "1    421\n",
      "0    405\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n",
      "Real Test Dataset Lacto_Binary Counts:\n",
      "Lacto_Binary\n",
      "0    433\n",
      "1    393\n",
      "Name: count, dtype: int64\n",
      "Unique values: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# ------ OBS THIS IS JUST TO CHECK THAT THE DIFFERENT DATA SETS ARE COMPATIBLE (SO Y IS 0/1 INT AND NOT FLOATS)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd().parents[1]\n",
    "\n",
    "# Function to load datasets and ensure integer type for Lacto_Binary\n",
    "def load_and_verify(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data['Lacto_Binary'] = data['Lacto_Binary'].astype(int)  # Ensure integer type\n",
    "    return data\n",
    "\n",
    "# BASELINE DATA \n",
    "baseline_path_train = root / 'data' / 'baseline_demographic' / 'train.csv'\n",
    "baseline_path_val = root / 'data' / 'baseline_demographic' / 'val.csv'\n",
    "baseline_path_test = root / 'data' / 'baseline_demographic' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_baseline = load_and_verify(baseline_path_train)\n",
    "print(\"Baseline Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_baseline = load_and_verify(baseline_path_val)\n",
    "print(\"Baseline Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_baseline['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_baseline = load_and_verify(baseline_path_test)\n",
    "print(\"Baseline Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_baseline['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_baseline['Lacto_Binary'].unique())\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "\n",
    "# REAL DATA \n",
    "path_train = root / 'data' / 'reduced_0_1' / 'train.csv'\n",
    "path_val = root / 'data' / 'reduced_0_1' / 'val.csv'\n",
    "path_test = root / 'data' / 'reduced_0_1' / 'test.csv'\n",
    "\n",
    "# For the training dataset\n",
    "train_data_real = load_and_verify(path_train)\n",
    "print(\"Real Training Dataset Lacto_Binary Counts:\")\n",
    "print(train_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", train_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the validation dataset\n",
    "val_data_real = load_and_verify(path_val)\n",
    "print(\"Real Validation Dataset Lacto_Binary Counts:\")\n",
    "print(val_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", val_data_real['Lacto_Binary'].unique())\n",
    "\n",
    "# For the test dataset\n",
    "test_data_real = load_and_verify(path_test)\n",
    "print(\"Real Test Dataset Lacto_Binary Counts:\")\n",
    "print(test_data_real['Lacto_Binary'].value_counts())\n",
    "print(\"Unique values:\", test_data_real['Lacto_Binary'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Forest_Basic - train - Accuracy: 1.0, ROC_AUC: 0.9999999999999999, PRC_AUC: 1.0\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1912.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}}\n",
      "Random_Forest_Basic - val - Accuracy: 0.8910411622276029, ROC_AUC: 0.9510454238878625, PRC_AUC: 0.9583633182553395\n",
      "{'0.0': {'precision': 0.8637413394919169, 'recall': 0.9234567901234568, 'f1-score': 0.8926014319809069, 'support': 405.0}, '1.0': {'precision': 0.9211195928753181, 'recall': 0.8598574821852731, 'f1-score': 0.8894348894348895, 'support': 421.0}, 'accuracy': 0.8910411622276029, 'macro avg': {'precision': 0.8924304661836175, 'recall': 0.891657136154365, 'f1-score': 0.8910181607078982, 'support': 826.0}, 'weighted avg': {'precision': 0.8929861877660232, 'recall': 0.8910411622276029, 'f1-score': 0.8909874920149585, 'support': 826.0}}\n",
      "Random_Forest_Basic - test - Accuracy: 0.9043583535108959, ROC_AUC: 0.9625019833224617, PRC_AUC: 0.9660777876489608\n",
      "{'0.0': {'precision': 0.8915929203539823, 'recall': 0.930715935334873, 'f1-score': 0.9107344632768362, 'support': 433.0}, '1.0': {'precision': 0.9197860962566845, 'recall': 0.8753180661577609, 'f1-score': 0.8970013037809648, 'support': 393.0}, 'accuracy': 0.9043583535108959, 'macro avg': {'precision': 0.9056895083053333, 'recall': 0.903017000746317, 'f1-score': 0.9038678835289005, 'support': 826.0}, 'weighted avg': {'precision': 0.9050068648210066, 'recall': 0.9043583535108959, 'f1-score': 0.9042004055505922, 'support': 826.0}}\n",
      "Best parameters: {'n_estimators': 10, 'min_samples_split': 2, 'max_features': None, 'max_depth': None}\n",
      "Random_Forest_Optimized - train - Accuracy: 0.9971428571428571, ROC_AUC: 0.9999828631078064, PRC_AUC: 0.9999826622414777\n",
      "{'0.0': {'precision': 0.9958826556870818, 'recall': 0.9984520123839009, 'f1-score': 0.9971656789487245, 'support': 1938.0}, '1.0': {'precision': 0.9984268484530676, 'recall': 0.99581589958159, 'f1-score': 0.9971196648337262, 'support': 1912.0}, 'accuracy': 0.9971428571428571, 'macro avg': {'precision': 0.9971547520700748, 'recall': 0.9971339559827455, 'f1-score': 0.9971426718912253, 'support': 3850.0}, 'weighted avg': {'precision': 0.9971461612893064, 'recall': 0.9971428571428571, 'f1-score': 0.9971428272635617, 'support': 3850.0}}\n",
      "Random_Forest_Optimized - val - Accuracy: 0.9891041162227603, ROC_AUC: 0.9972082930119353, PRC_AUC: 0.998443605264625\n",
      "{'0.0': {'precision': 0.9876847290640394, 'recall': 0.9901234567901235, 'f1-score': 0.9889025893958077, 'support': 405.0}, '1.0': {'precision': 0.9904761904761905, 'recall': 0.9881235154394299, 'f1-score': 0.9892984542211652, 'support': 421.0}, 'accuracy': 0.9891041162227603, 'macro avg': {'precision': 0.9890804597701149, 'recall': 0.9891234861147766, 'f1-score': 0.9891005218084865, 'support': 826.0}, 'weighted avg': {'precision': 0.9891074957159954, 'recall': 0.9891041162227603, 'f1-score': 0.9891043558503786, 'support': 826.0}}\n",
      "Random_Forest_Optimized - test - Accuracy: 0.9891041162227603, ROC_AUC: 0.9969735968360867, PRC_AUC: 0.9981603879983652\n",
      "{'0.0': {'precision': 0.9840182648401826, 'recall': 0.9953810623556582, 'f1-score': 0.9896670493685419, 'support': 433.0}, '1.0': {'precision': 0.9948453608247423, 'recall': 0.9821882951653944, 'f1-score': 0.9884763124199744, 'support': 393.0}, 'accuracy': 0.9891041162227603, 'macro avg': {'precision': 0.9894318128324624, 'recall': 0.9887846787605263, 'f1-score': 0.9890716808942581, 'support': 826.0}, 'weighted avg': {'precision': 0.9891696555447006, 'recall': 0.9891041162227603, 'f1-score': 0.9891005122973711, 'support': 826.0}}\n",
      "XGBoost_Basic - train - Accuracy: 1.0, ROC_AUC: 1.0, PRC_AUC: 1.0\n",
      "{'0.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1912.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3850.0}}\n",
      "XGBoost_Basic - val - Accuracy: 0.9745762711864406, ROC_AUC: 0.9960880912583209, PRC_AUC: 0.9969763133943088\n",
      "{'0.0': {'precision': 0.9528301886792453, 'recall': 0.9975308641975309, 'f1-score': 0.9746682750301568, 'support': 405.0}, '1.0': {'precision': 0.9975124378109452, 'recall': 0.9524940617577197, 'f1-score': 0.9744835965978129, 'support': 421.0}, 'accuracy': 0.9745762711864406, 'macro avg': {'precision': 0.9751713132450952, 'recall': 0.9750124629776253, 'f1-score': 0.9745759358139849, 'support': 826.0}, 'weighted avg': {'precision': 0.975604071105935, 'recall': 0.9745762711864406, 'f1-score': 0.9745741471608871, 'support': 826.0}}\n",
      "XGBoost_Basic - test - Accuracy: 0.9782082324455206, ROC_AUC: 0.9942997843320465, PRC_AUC: 0.9958143186095364\n",
      "{'0.0': {'precision': 0.9683972911963883, 'recall': 0.9907621247113164, 'f1-score': 0.9794520547945206, 'support': 433.0}, '1.0': {'precision': 0.9895561357702349, 'recall': 0.9643765903307888, 'f1-score': 0.9768041237113402, 'support': 393.0}, 'accuracy': 0.9782082324455206, 'macro avg': {'precision': 0.9789767134833116, 'recall': 0.9775693575210527, 'f1-score': 0.9781280892529304, 'support': 826.0}, 'weighted avg': {'precision': 0.9784643927914509, 'recall': 0.9782082324455206, 'f1-score': 0.9781922038070026, 'support': 826.0}}\n",
      "Best hyperparameters:\n",
      " {'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "XGBoost_Optimized - train - Accuracy: 0.9976623376623377, ROC_AUC: 0.9999989205107279, PRC_AUC: 0.9999989078306699\n",
      "{'0.0': {'precision': 0.9953775038520801, 'recall': 1.0, 'f1-score': 0.9976833976833976, 'support': 1938.0}, '1.0': {'precision': 1.0, 'recall': 0.9952928870292888, 'f1-score': 0.9976408912188729, 'support': 1912.0}, 'accuracy': 0.9976623376623377, 'macro avg': {'precision': 0.99768875192604, 'recall': 0.9976464435146444, 'f1-score': 0.9976621444511353, 'support': 3850.0}, 'weighted avg': {'precision': 0.9976731434974887, 'recall': 0.9976623376623377, 'f1-score': 0.997662287979457, 'support': 3850.0}}\n",
      "XGBoost_Optimized - val - Accuracy: 0.9806295399515739, ROC_AUC: 0.9950910530482978, PRC_AUC: 0.9966310161195652\n",
      "{'0.0': {'precision': 0.9619952494061758, 'recall': 1.0, 'f1-score': 0.9806295399515739, 'support': 405.0}, '1.0': {'precision': 1.0, 'recall': 0.9619952494061758, 'f1-score': 0.9806295399515739, 'support': 421.0}, 'accuracy': 0.9806295399515739, 'macro avg': {'precision': 0.9809976247030878, 'recall': 0.9809976247030878, 'f1-score': 0.9806295399515739, 'support': 826.0}, 'weighted avg': {'precision': 0.981365709454602, 'recall': 0.9806295399515739, 'f1-score': 0.9806295399515739, 'support': 826.0}}\n",
      "XGBoost_Optimized - test - Accuracy: 0.9842615012106537, ROC_AUC: 0.9967267833741751, PRC_AUC: 0.9973828171756501\n",
      "{'0.0': {'precision': 0.9772727272727273, 'recall': 0.9930715935334873, 'f1-score': 0.9851088201603666, 'support': 433.0}, '1.0': {'precision': 0.9922279792746114, 'recall': 0.9745547073791349, 'f1-score': 0.9833119383825417, 'support': 393.0}, 'accuracy': 0.9842615012106537, 'macro avg': {'precision': 0.9847503532736693, 'recall': 0.983813150456311, 'f1-score': 0.9842103792714542, 'support': 826.0}, 'weighted avg': {'precision': 0.9843882406343986, 'recall': 0.9842615012106537, 'f1-score': 0.9842538873048154, 'support': 826.0}}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4997 - loss: 1.6202 - val_accuracy: 0.5400 - val_loss: 0.8644\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6254 - loss: 0.7080 - val_accuracy: 0.6828 - val_loss: 0.6167\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6548 - loss: 0.6275 - val_accuracy: 0.6755 - val_loss: 0.6067\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 0.6193 - val_accuracy: 0.6041 - val_loss: 0.7688\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6819 - loss: 0.6430 - val_accuracy: 0.6017 - val_loss: 0.7895\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.6443 - val_accuracy: 0.6804 - val_loss: 0.6080\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6625 - loss: 0.7823 - val_accuracy: 0.6804 - val_loss: 0.6258\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7040 - loss: 0.5777 - val_accuracy: 0.5835 - val_loss: 1.1006\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6581 - loss: 0.7417 - val_accuracy: 0.5412 - val_loss: 1.9200\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6777 - loss: 0.7930 - val_accuracy: 0.6271 - val_loss: 0.8488\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6582 - loss: 0.7681 \n",
      "Neural Network Test accuracy: 0.6634382605552673\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression_Basic - train - Accuracy: 0.8244155844155844, ROC_AUC: 0.9084180192667246, PRC_AUC: 0.9177791409366127\n",
      "{'0.0': {'precision': 0.7873406193078324, 'recall': 0.8921568627450981, 'f1-score': 0.8364779874213837, 'support': 1938.0}, '1.0': {'precision': 0.873639661426844, 'recall': 0.7557531380753139, 'f1-score': 0.8104318564217611, 'support': 1912.0}, 'accuracy': 0.8244155844155844, 'macro avg': {'precision': 0.8304901403673381, 'recall': 0.823955000410206, 'f1-score': 0.8234549219215723, 'support': 3850.0}, 'weighted avg': {'precision': 0.8301987410043389, 'recall': 0.8244155844155844, 'f1-score': 0.8235428698963764, 'support': 3850.0}}\n",
      "Logistic_Regression_Basic - val - Accuracy: 0.7554479418886199, ROC_AUC: 0.8254244743555907, PRC_AUC: 0.8356575264364974\n",
      "{'0.0': {'precision': 0.7084188911704312, 'recall': 0.8518518518518519, 'f1-score': 0.773542600896861, 'support': 405.0}, '1.0': {'precision': 0.8230088495575221, 'recall': 0.66270783847981, 'f1-score': 0.7342105263157894, 'support': 421.0}, 'accuracy': 0.7554479418886199, 'macro avg': {'precision': 0.7657138703639766, 'recall': 0.7572798451658309, 'f1-score': 0.7538765636063252, 'support': 826.0}, 'weighted avg': {'precision': 0.7668237004694206, 'recall': 0.7554479418886199, 'f1-score': 0.7534956234166781, 'support': 826.0}}\n",
      "Logistic_Regression_Basic - test - Accuracy: 0.7796610169491526, ROC_AUC: 0.8514535549953282, PRC_AUC: 0.8574760079627711\n",
      "{'0.0': {'precision': 0.7664543524416136, 'recall': 0.8337182448036952, 'f1-score': 0.7986725663716814, 'support': 433.0}, '1.0': {'precision': 0.7971830985915493, 'recall': 0.7201017811704835, 'f1-score': 0.7566844919786097, 'support': 393.0}, 'accuracy': 0.7796610169491526, 'macro avg': {'precision': 0.7818187255165815, 'recall': 0.7769100129870894, 'f1-score': 0.7776785291751456, 'support': 826.0}, 'weighted avg': {'precision': 0.7810746880795372, 'recall': 0.7796610169491526, 'f1-score': 0.7786951895720722, 'support': 826.0}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/MB-LM-24/src/Python/analysis_modular.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         run_all_models(data_dir, results_dir)\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# run the main function using \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m main()\n",
      "\u001b[1;32m/work/MB-LM-24/src/Python/analysis_modular.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m results_dir \u001b[39m=\u001b[39m root \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_reports\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m data_dir \u001b[39min\u001b[39;00m data_dirs:\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     run_all_models(data_dir, results_dir)\n",
      "\u001b[1;32m/work/MB-LM-24/src/Python/analysis_modular.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m basic_results, basic_roc_data, basic_prc_data \u001b[39m=\u001b[39m evaluate_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Elastic Net Logistic Regression\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m enet_results, enet_roc_data, enet_prc_data \u001b[39m=\u001b[39m evaluate_elastic_net_logistic_regression(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Majority Class Classifier\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m majority_results, majority_roc_data, majority_prc_data \u001b[39m=\u001b[39m evaluate_majority_class_classifier(X_train, y_train, X_val, y_val, X_test, y_test, dataset_results_dir)\n",
      "\u001b[1;32m/work/MB-LM-24/src/Python/analysis_modular.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_elastic_net_logistic_regression\u001b[39m(X_train, y_train, X_val, y_val, X_test, y_test, results_dir):\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# train an Elastic Net Logistic Regression model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     elastic_net_lr \u001b[39m=\u001b[39m LogisticRegressionCV(cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, penalty\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39melasticnet\u001b[39m\u001b[39m'\u001b[39m, solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m'\u001b[39m, l1_ratios\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m], random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     elastic_net_lr\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     splits \u001b[39m=\u001b[39m [(X_train, y_train, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m), (X_val, y_val, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m), (X_test, y_test, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5051102-0.cloud.sdu.dk/work/MB-LM-24/src/Python/analysis_modular.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     enet_results, enet_roc_data, enet_prc_data, test_roc_data, test_prc_data \u001b[39m=\u001b[39m evaluate_classification(elastic_net_lr, splits, model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mElastic_Net_Logistic_Regression\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1991\u001b[0m, in \u001b[0;36mLogisticRegressionCV.fit\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1989\u001b[0m     prefer \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprocesses\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1991\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[1;32m   1992\u001b[0m     path_func(\n\u001b[1;32m   1993\u001b[0m         X,\n\u001b[1;32m   1994\u001b[0m         y,\n\u001b[1;32m   1995\u001b[0m         train,\n\u001b[1;32m   1996\u001b[0m         test,\n\u001b[1;32m   1997\u001b[0m         pos_class\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m   1998\u001b[0m         Cs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCs,\n\u001b[1;32m   1999\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   2000\u001b[0m         penalty\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m   2001\u001b[0m         dual\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m   2002\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   2003\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   2004\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   2005\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   2006\u001b[0m         class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2007\u001b[0m         scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring,\n\u001b[1;32m   2008\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   2009\u001b[0m         intercept_scaling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m   2010\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   2011\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   2012\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   2013\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49ml1_ratio,\n\u001b[1;32m   2014\u001b[0m         score_params\u001b[39m=\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49mscorer\u001b[39m.\u001b[39;49mscore,\n\u001b[1;32m   2015\u001b[0m     )\n\u001b[1;32m   2016\u001b[0m     \u001b[39mfor\u001b[39;49;00m label \u001b[39min\u001b[39;49;00m iter_encoded_labels\n\u001b[1;32m   2017\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m folds\n\u001b[1;32m   2018\u001b[0m     \u001b[39mfor\u001b[39;49;00m l1_ratio \u001b[39min\u001b[39;49;00m l1_ratios_\n\u001b[1;32m   2019\u001b[0m )\n\u001b[1;32m   2021\u001b[0m \u001b[39m# _log_reg_scoring_path will output different shapes depending on the\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[39m# multi_class param, so we need to reshape the outputs accordingly.\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m \u001b[39m# Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[39m#  (n_classes, n_folds, n_Cs . n_l1_ratios) or\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[39m#  (1, n_folds, n_Cs . n_l1_ratios)\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m coefs_paths, Cs, scores, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:748\u001b[0m, in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio, score_params)\u001b[0m\n\u001b[1;32m    745\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n\u001b[1;32m    746\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight[train]\n\u001b[0;32m--> 748\u001b[0m coefs, Cs, n_iter \u001b[39m=\u001b[39m _logistic_regression_path(\n\u001b[1;32m    749\u001b[0m     X_train,\n\u001b[1;32m    750\u001b[0m     y_train,\n\u001b[1;32m    751\u001b[0m     Cs\u001b[39m=\u001b[39;49mCs,\n\u001b[1;32m    752\u001b[0m     l1_ratio\u001b[39m=\u001b[39;49ml1_ratio,\n\u001b[1;32m    753\u001b[0m     fit_intercept\u001b[39m=\u001b[39;49mfit_intercept,\n\u001b[1;32m    754\u001b[0m     solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m    755\u001b[0m     max_iter\u001b[39m=\u001b[39;49mmax_iter,\n\u001b[1;32m    756\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    757\u001b[0m     pos_class\u001b[39m=\u001b[39;49mpos_class,\n\u001b[1;32m    758\u001b[0m     multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m    759\u001b[0m     tol\u001b[39m=\u001b[39;49mtol,\n\u001b[1;32m    760\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    761\u001b[0m     dual\u001b[39m=\u001b[39;49mdual,\n\u001b[1;32m    762\u001b[0m     penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m    763\u001b[0m     intercept_scaling\u001b[39m=\u001b[39;49mintercept_scaling,\n\u001b[1;32m    764\u001b[0m     random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    765\u001b[0m     check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    766\u001b[0m     max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m    767\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    768\u001b[0m )\n\u001b[1;32m    770\u001b[0m log_reg \u001b[39m=\u001b[39m LogisticRegression(solver\u001b[39m=\u001b[39msolver, multi_class\u001b[39m=\u001b[39mmulti_class)\n\u001b[1;32m    772\u001b[0m \u001b[39m# The score method of Logistic Regression has a classes_ attribute.\u001b[39;00m\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:547\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    544\u001b[0m         alpha \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m l1_ratio)\n\u001b[1;32m    545\u001b[0m         beta \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m l1_ratio\n\u001b[0;32m--> 547\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[39m=\u001b[39m sag_solver(\n\u001b[1;32m    548\u001b[0m         X,\n\u001b[1;32m    549\u001b[0m         target,\n\u001b[1;32m    550\u001b[0m         sample_weight,\n\u001b[1;32m    551\u001b[0m         loss,\n\u001b[1;32m    552\u001b[0m         alpha,\n\u001b[1;32m    553\u001b[0m         beta,\n\u001b[1;32m    554\u001b[0m         max_iter,\n\u001b[1;32m    555\u001b[0m         tol,\n\u001b[1;32m    556\u001b[0m         verbose,\n\u001b[1;32m    557\u001b[0m         random_state,\n\u001b[1;32m    558\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    559\u001b[0m         max_squared_sum,\n\u001b[1;32m    560\u001b[0m         warm_start_sag,\n\u001b[1;32m    561\u001b[0m         is_saga\u001b[39m=\u001b[39;49m(solver \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msolver must be one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m solver\n\u001b[1;32m    568\u001b[0m     )\n",
      "File \u001b[0;32m/work/venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:324\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent sag implementation does not handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    323\u001b[0m sag \u001b[39m=\u001b[39m sag64 \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64 \u001b[39melse\u001b[39;00m sag32\n\u001b[0;32m--> 324\u001b[0m num_seen, n_iter_ \u001b[39m=\u001b[39m sag(\n\u001b[1;32m    325\u001b[0m     dataset,\n\u001b[1;32m    326\u001b[0m     coef_init,\n\u001b[1;32m    327\u001b[0m     intercept_init,\n\u001b[1;32m    328\u001b[0m     n_samples,\n\u001b[1;32m    329\u001b[0m     n_features,\n\u001b[1;32m    330\u001b[0m     n_classes,\n\u001b[1;32m    331\u001b[0m     tol,\n\u001b[1;32m    332\u001b[0m     max_iter,\n\u001b[1;32m    333\u001b[0m     loss,\n\u001b[1;32m    334\u001b[0m     step_size,\n\u001b[1;32m    335\u001b[0m     alpha_scaled,\n\u001b[1;32m    336\u001b[0m     beta_scaled,\n\u001b[1;32m    337\u001b[0m     sum_gradient_init,\n\u001b[1;32m    338\u001b[0m     gradient_memory_init,\n\u001b[1;32m    339\u001b[0m     seen_init,\n\u001b[1;32m    340\u001b[0m     num_seen_init,\n\u001b[1;32m    341\u001b[0m     fit_intercept,\n\u001b[1;32m    342\u001b[0m     intercept_sum_gradient,\n\u001b[1;32m    343\u001b[0m     intercept_decay,\n\u001b[1;32m    344\u001b[0m     is_saga,\n\u001b[1;32m    345\u001b[0m     verbose,\n\u001b[1;32m    346\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m n_iter_ \u001b[39m==\u001b[39m max_iter:\n\u001b[1;32m    349\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    351\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    352\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def main():\n",
    "    root = Path.cwd().parents[1]\n",
    "    data_dirs = [root / \"data\" / \"reduced_0_1\", root / \"data\" / \"reduced_0_1_PCA\", root / \"data\" / \"reduced_0_1_SVD\", root / \"data\" / \"baseline_demographic\"]\n",
    "    results_dir = root / \"results\" / \"model_reports\"\n",
    "    for data_dir in data_dirs:\n",
    "        run_all_models(data_dir, results_dir)\n",
    "\n",
    "# run the main function using \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
