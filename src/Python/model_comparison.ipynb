{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root = Path.cwd().parents[1]\n",
    "results_root = root / \"results/model_reports/\"\n",
    "\n",
    "# Function to extract data\n",
    "def extract_metrics(section):\n",
    "    metrics = {}\n",
    "    accuracy = section[1]\n",
    "    report = section[2]\n",
    "\n",
    "    # Extract accuracy\n",
    "    metrics['Accuracy'] = float(accuracy)\n",
    "    \n",
    "    # Extract individual class metrics\n",
    "    class_metrics = re.findall(r'(\\d.\\d): \\{\\'precision\\': ([\\d.]+), \\'recall\\': ([\\d.]+), \\'f1-score\\': ([\\d.]+), \\'support\\': ([\\d.]+)\\}', report)\n",
    "    for metric in class_metrics:\n",
    "        class_id = metric[0]\n",
    "        metrics[f'Class_{class_id}_precision'] = float(metric[1])\n",
    "        metrics[f'Class_{class_id}_recall'] = float(metric[2])\n",
    "        metrics[f'Class_{class_id}_f1-score'] = float(metric[3])\n",
    "        metrics[f'Class_{class_id}_support'] = float(metric[4])\n",
    "    \n",
    "    # Extract macro avg metrics\n",
    "    macro_avg = re.search(r'macro avg: \\{\\'precision\\': ([\\d.]+), \\'recall\\': ([\\d.]+), \\'f1-score\\': ([\\d.]+), \\'support\\': ([\\d.]+)\\}', report)\n",
    "    metrics['macro_avg_precision'] = float(macro_avg.group(1))\n",
    "    metrics['macro_avg_recall'] = float(macro_avg.group(2))\n",
    "    metrics['macro_avg_f1-score'] = float(macro_avg.group(3))\n",
    "    metrics['macro_avg_support'] = float(macro_avg.group(4))\n",
    "    \n",
    "    # Extract weighted avg metrics\n",
    "    weighted_avg = re.search(r'weighted avg: \\{\\'precision\\': ([\\d.]+), \\'recall\\': ([\\d.]+), \\'f1-score\\': ([\\d.]+), \\'support\\': ([\\d.]+)\\}', report)\n",
    "    metrics['weighted_avg_precision'] = float(weighted_avg.group(1))\n",
    "    metrics['weighted_avg_recall'] = float(weighted_avg.group(2))\n",
    "    metrics['weighted_avg_f1-score'] = float(weighted_avg.group(3))\n",
    "    metrics['weighted_avg_support'] = float(weighted_avg.group(4))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# List to store all data\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all directories in the results/model_reports directory\n",
    "for results in results_root.glob(\"*/\"):\n",
    "    # Extract the directory name\n",
    "    directory_name = results.name\n",
    "    \n",
    "    # Iterate over all txt files in the directory\n",
    "    for txt_file in results.glob(\"*.txt\"):\n",
    "        # Read the text file\n",
    "        with open(txt_file, 'r') as file:\n",
    "            data = file.read()\n",
    "        \n",
    "        # Extract the model name from the file path\n",
    "        model_name = txt_file.stem.replace('_results', '')\n",
    "\n",
    "        # Generalize the regular expression to split the data into sections\n",
    "        sections = re.split(r'(\\w+) - (\\w+) - Accuracy: ([\\d.]+)', data)[1:]\n",
    "\n",
    "        # Process each section and store the results in a list\n",
    "        for i in range(0, len(sections), 4):\n",
    "            classifier_name = sections[i]\n",
    "            split_type = sections[i+1]\n",
    "            accuracy = sections[i+2]\n",
    "            section_data = sections[i+3]\n",
    "            section_metrics = extract_metrics([split_type, accuracy, section_data])\n",
    "            section_metrics['Split'] = split_type\n",
    "            section_metrics['Model'] = model_name\n",
    "            section_metrics['Directory'] = directory_name\n",
    "            all_data.append(section_metrics)\n",
    "\n",
    "# Create DataFrame\n",
    "combined_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Ensure 'Directory' is the first column, 'Model' is the second column, and 'Split' is the third column\n",
    "columns = ['Directory', 'Model', 'Split'] + [col for col in combined_df.columns if col not in ['Directory', 'Model', 'Split']]\n",
    "combined_df = combined_df[columns]\n",
    "combined_df\n",
    "\n",
    "\n",
    "for directory in combined_df['Directory'].unique():\n",
    "    # Filter the DataFrame for the current directory\n",
    "    subset_df = combined_df[combined_df['Directory'] == directory]\n",
    "    \n",
    "    # Sort the DataFrame by 'Accuracy' for the first plot\n",
    "    sorted_df_accuracy = subset_df.sort_values(by='Accuracy', ascending=False)\n",
    "    \n",
    "    # Sort the DataFrame by 'weighted_avg_f1-score' for the second plot\n",
    "    sorted_df_f1 = subset_df.sort_values(by='weighted_avg_f1-score', ascending=False)\n",
    "    \n",
    "    # Create subplots with 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Create the scatter plot for Accuracy\n",
    "    sns.scatterplot(ax=axes[0], data=sorted_df_accuracy, \n",
    "                    y='Model', \n",
    "                    x='Accuracy', \n",
    "                    marker='s', \n",
    "                    hue='Split', palette=['darkorange', 'grey', 'darkred'])\n",
    "    \n",
    "    # Set plot title for Accuracy\n",
    "    axes[0].set_title(f'Performance Metrics (Accuracy) for {directory}')\n",
    "    \n",
    "    # Create the scatter plot for weighted_avg_f1-score\n",
    "    sns.scatterplot(ax=axes[1], data=sorted_df_f1, \n",
    "                    y='Model', \n",
    "                    x='weighted_avg_f1-score', \n",
    "                    marker='s', \n",
    "                    hue='Split', palette=['darkorange', 'grey', 'darkred'])\n",
    "    \n",
    "    # Set plot title for weighted_avg_f1-score\n",
    "    axes[1].set_title(f'Performance Metrics (Weighted Avg F1-Score) for {directory}')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
