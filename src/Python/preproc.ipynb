{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Modelling (Data Splits, including thresholding and PCA)\n",
    "The current notebook is used to create datasets that can be used for modelling. Updated 21st of May!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import pickle as pkl\n",
    "from sklearn import datasets\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make the baseline data (with one-hot encoded categorical variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lacto_Binary\n",
      "0    2776\n",
      "1    2726\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3008/2634217379.py:10: DtypeWarning: Columns (20,21,22,25,26,28,29,30,31,32,34,37,45,46,47,48,49,50,55,56,57,58,60,72,82,83) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  demographic_data = pd.read_csv(baseline_file_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>study_name</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>body_site</th>\n",
       "      <th>antibiotics_current_use</th>\n",
       "      <th>study_condition</th>\n",
       "      <th>disease</th>\n",
       "      <th>age</th>\n",
       "      <th>age_category</th>\n",
       "      <th>...</th>\n",
       "      <th>fasting_glucose</th>\n",
       "      <th>ajcc</th>\n",
       "      <th>c_section_type</th>\n",
       "      <th>zigosity</th>\n",
       "      <th>brinkman_index</th>\n",
       "      <th>alcohol_numeric</th>\n",
       "      <th>ALT</th>\n",
       "      <th>eGFR</th>\n",
       "      <th>LactoSum</th>\n",
       "      <th>Lacto_Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a00820d6-7ae6-11e9-a106-68b59976a384</td>\n",
       "      <td>ShaoY_2019</td>\n",
       "      <td>B01042_mo</td>\n",
       "      <td>stool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>control</td>\n",
       "      <td>healthy</td>\n",
       "      <td>32</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elective_CS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A01_02_1FE</td>\n",
       "      <td>PasolliE_2019</td>\n",
       "      <td>A01_02_1FE</td>\n",
       "      <td>stool</td>\n",
       "      <td>no</td>\n",
       "      <td>control</td>\n",
       "      <td>healthy</td>\n",
       "      <td>21</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A02_01_1FE</td>\n",
       "      <td>PasolliE_2019</td>\n",
       "      <td>A02_01_1FE</td>\n",
       "      <td>stool</td>\n",
       "      <td>no</td>\n",
       "      <td>control</td>\n",
       "      <td>healthy</td>\n",
       "      <td>24</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A03_01_1FE</td>\n",
       "      <td>PasolliE_2019</td>\n",
       "      <td>A03_01_1FE</td>\n",
       "      <td>stool</td>\n",
       "      <td>no</td>\n",
       "      <td>control</td>\n",
       "      <td>healthy</td>\n",
       "      <td>38</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A04_01_1FE</td>\n",
       "      <td>PasolliE_2019</td>\n",
       "      <td>A04_01_1FE</td>\n",
       "      <td>stool</td>\n",
       "      <td>no</td>\n",
       "      <td>control</td>\n",
       "      <td>healthy</td>\n",
       "      <td>34</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             sample_id     study_name  \\\n",
       "0           1  a00820d6-7ae6-11e9-a106-68b59976a384     ShaoY_2019   \n",
       "1           2                            A01_02_1FE  PasolliE_2019   \n",
       "2           3                            A02_01_1FE  PasolliE_2019   \n",
       "3           4                            A03_01_1FE  PasolliE_2019   \n",
       "4           5                            A04_01_1FE  PasolliE_2019   \n",
       "\n",
       "   subject_id body_site antibiotics_current_use study_condition  disease  age  \\\n",
       "0   B01042_mo     stool                     NaN         control  healthy   32   \n",
       "1  A01_02_1FE     stool                      no         control  healthy   21   \n",
       "2  A02_01_1FE     stool                      no         control  healthy   24   \n",
       "3  A03_01_1FE     stool                      no         control  healthy   38   \n",
       "4  A04_01_1FE     stool                      no         control  healthy   34   \n",
       "\n",
       "  age_category  ... fasting_glucose ajcc c_section_type zigosity  \\\n",
       "0        adult  ...             NaN  NaN    Elective_CS      NaN   \n",
       "1        adult  ...             NaN  NaN            NaN      NaN   \n",
       "2        adult  ...             NaN  NaN            NaN      NaN   \n",
       "3        adult  ...             NaN  NaN            NaN      NaN   \n",
       "4        adult  ...             NaN  NaN            NaN      NaN   \n",
       "\n",
       "  brinkman_index  alcohol_numeric  ALT  eGFR  LactoSum  Lacto_Binary  \n",
       "0            NaN              NaN  NaN   NaN   0.00000             0  \n",
       "1            NaN              NaN  NaN   NaN   0.00000             0  \n",
       "2            NaN              NaN  NaN   NaN   0.00000             0  \n",
       "3            NaN              NaN  NaN   NaN   0.00000             0  \n",
       "4            NaN              NaN  NaN   NaN   0.03775             1  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3168\n",
      "Validation set  | 0.47 | 0.53 |     680\n",
      "Test set        | 0.49 | 0.51 |     680\n"
     ]
    }
   ],
   "source": [
    "### TRYING TO DROP NA VALUES\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "root = Path.cwd().parents[1]\n",
    "baseline_file_path = root / \"data\" / \"raw\" / \"healthy_subset_df_with_meta.csv\"\n",
    "demographic_data = pd.read_csv(baseline_file_path)\n",
    "\n",
    "# calculate lactosum and create binary target variable \n",
    "lacto_columns = [col for col in demographic_data.columns if 'lactobacillus' in col.lower()]\n",
    "demographic_data['LactoSum'] = demographic_data[lacto_columns].sum(axis=1)\n",
    "demographic_data['Lacto_Binary'] = np.where(demographic_data['LactoSum'] > 0.01, 1, 0).astype(int)\n",
    "\n",
    "# keep only columns containing the string 'Lacto_Binary' or columns that don't contain the string '|'\n",
    "cols_to_keep = [col for col in demographic_data.columns if 'Lacto_Binary' in col or '|' not in col]\n",
    "demographic_data = demographic_data[cols_to_keep]\n",
    "\n",
    "# print values of Lacto_Binary to check, it should be 2776/2726\n",
    "print(demographic_data['Lacto_Binary'].value_counts())\n",
    "display(demographic_data.head())\n",
    "\n",
    "# drop LactoSum column\n",
    "demographic_data = demographic_data.drop(columns=['LactoSum'])\n",
    "\n",
    "# save the cleaned data \n",
    "demographic_data.to_csv(root / \"data\" / \"baseline_demographic\" / \"healthy_subset_df_with_meta_lactobinary.csv\", index=False)\n",
    "\n",
    "# ---- Partition data \n",
    "selected_variables = [\n",
    "    'age', 'gender', 'country', 'BMI', 'diet', 'smoker', 'alcohol'\n",
    "]\n",
    "\n",
    "# drop all columns that are not 'Lacto_Binary' or in the selected_variables list\n",
    "demographic_data = demographic_data[[col for col in demographic_data.columns if 'Lacto_Binary' in col or col in selected_variables]]\n",
    "\n",
    "# One-hot encode categorical variables with 0/1 encoding\n",
    "categorical_vars = ['gender', 'country', 'diet', 'smoker', 'alcohol']\n",
    "demographic_data = pd.get_dummies(demographic_data, columns=categorical_vars, drop_first=True, dtype=int)\n",
    "\n",
    "# Update selected_variables after one-hot encoding\n",
    "encoded_variables = list(demographic_data.columns)\n",
    "selected_variables = [var for var in encoded_variables if any(orig_var in var for orig_var in ['age', 'BMI', 'gender', 'country', 'diet', 'smoker', 'alcohol'])]\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "demographic_data = demographic_data.dropna()\n",
    "\n",
    "# split data into features (X) and target (y)\n",
    "X = demographic_data[selected_variables]\n",
    "y = demographic_data['Lacto_Binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)\n",
    "\n",
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test} # create a dictionary with the datasets\n",
    "\n",
    "# open a file to write\n",
    "with open('../../data/baseline_demographic/dataset_distribution.txt', 'w') as file:\n",
    "    # print table headers and write to file\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    # loop through each dataset, print proportions and total counts, and write to file\n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)  # total number of observations\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "for x, y, name in zip([X_train, X_val, X_test], \n",
    "                      [y_train, y_val, y_test], \n",
    "                      ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.values.reshape(-1, 1)]) # combine X and y for each dataset \n",
    "    column_names = list(X.columns) + ['Lacto_Binary'] # construct list of column names, add Lacto_Binary to make sure it is the last column \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/baseline_demographic/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in X_train with at least one NaN value: 0\n",
      "Number of rows in X_val with at least one NaN value: 0\n",
      "Number of rows in X_test with at least one NaN value: 0\n",
      "-----------------------------------------------------------------------\n",
      "Total number of rows in X_train: 3168\n",
      "Total number of rows in X_val: 680\n",
      "Total number of rows in X_test: 680\n",
      "-----------------------------------------------------------------------\n",
      "Number of rows left in X_train if I remove rows with at least one NaN value: 3168\n",
      "Number of rows left in X_val if I remove rows with at least one NaN value: 680\n",
      "Number of rows left in X_test if I remove rows with at least one NaN value: 680\n"
     ]
    }
   ],
   "source": [
    "# count how many rows in X_train has at least one 'NaN' value. \n",
    "nan_train = X_train.isnull().any(axis=1).sum()\n",
    "print(f'Number of rows in X_train with at least one NaN value: {nan_train}')\n",
    "\n",
    "nan_val = X_val.isnull().any(axis=1).sum()\n",
    "print(f'Number of rows in X_val with at least one NaN value: {nan_val}')\n",
    "\n",
    "nan_test = X_test.isnull().any(axis=1).sum()\n",
    "print(f'Number of rows in X_test with at least one NaN value: {nan_test}')\n",
    "\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "# print total number of rows in each dataset\n",
    "print(f'Total number of rows in X_train: {X_train.shape[0]}')\n",
    "print(f'Total number of rows in X_val: {X_val.shape[0]}')\n",
    "print(f'Total number of rows in X_test: {X_test.shape[0]}')\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "# The number of rows left in each dataset if I remove rows with at least one NaN value\n",
    "left_train = X_train.dropna().shape[0]\n",
    "left_val = X_val.dropna().shape[0]\n",
    "left_test = X_test.dropna().shape[0]\n",
    "\n",
    "print(f'Number of rows left in X_train if I remove rows with at least one NaN value: {left_train}')\n",
    "print(f'Number of rows left in X_val if I remove rows with at least one NaN value: {left_val}')\n",
    "print(f'Number of rows left in X_test if I remove rows with at least one NaN value: {left_test}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make the thresholded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parents[1]\n",
    "data_path = root / \"data\" / \"raw\" / \"healthy_subset_df.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "lacto_columns = [col for col in data.columns if 'lactobacillus' in col.lower()]\n",
    "data['LactoSum'] = data[lacto_columns].sum(axis=1)\n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0).astype(int) # astype(int) converts True/False to 1/0 \n",
    "# drop the LactoSum column\n",
    "data = data.drop(columns=['LactoSum'])\n",
    "\n",
    "# ------------- PARTITION TEMPORARY DATA (BEFORE THRESHOLDING)\n",
    "X = data.drop(columns=['sample_id', 'Lacto_Binary'])\n",
    "y = data['Lacto_Binary']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# ------ FIGURING OUT WHICH COLS ARE BWLOW THRESHOLD ---------- # \n",
    "species_columns = [col for col in X_train.columns if '|' in col]\n",
    "# calculate sum for each column\n",
    "columns_sums = X_train[species_columns].sum()\n",
    "# calculate total sum across all columns and rows\n",
    "total_sum = columns_sums.sum()\n",
    "# calculate percentage for each column\n",
    "columns_percentages = (columns_sums / total_sum) * 100\n",
    "# find columns where percentage is 0.1% or more\n",
    "filtered_columns = columns_percentages[columns_percentages >= 0.1]\n",
    "\n",
    "# filter X_train to keep only columns with percentage >= 0.1%\n",
    "X_train_filtered = X_train[filtered_columns.index]\n",
    "\n",
    "# Remove filtered columns from X_val and X_test\n",
    "X_val_filtered = X_val[filtered_columns.index]\n",
    "X_test_filtered = X_test[filtered_columns.index]\n",
    "\n",
    "\n",
    "# ------ SAVE THE TRHESHOLDED DATA ------ # \n",
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test} # create a dictionary with the datasets\n",
    "output_dir = root / 'data' / 'reduced_0_1'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# open a file to write\n",
    "with open(output_dir / 'dataset_distribution.txt', 'w') as file:\n",
    "    # print table headers and write to file\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    # loop through each dataset, print proportions and total counts, and write to file\n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)  # total number of observation\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "\n",
    "for x, y, name in zip([X_train_filtered, X_val_filtered, X_test_filtered], [y_train, y_val, y_test], ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.values.reshape(-1, 1)]) # combine X and y for each dataset \n",
    "    column_names = list(x.columns) + ['Lacto_Binary'] # construct list of column names, add Lacto_Binary to make sure it is the last column \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(output_dir /f'{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholded + PCA \n",
    "I do this because the data is very sparse and has a lot of features. I want to reduce the number of features to make the model more interpretable and to reduce the risk of overfitting.\n",
    "I do this by first thresholding the data and then applying PCA. PCA decrease the number of features by selecting dimension of features which have most of the variance. Data is still on the same scale and sum to 1, and therefore I have chosen not to scale the data before applying PCA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31394337 0.09736937 0.05340137 0.04002439 0.03690475 0.03289054\n",
      " 0.02757451 0.02682614 0.02580687 0.0253191 ]\n",
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parents[1]\n",
    "data_path = root / \"data\" / \"raw\" / \"healthy_subset_df.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "lacto_columns = [col for col in data.columns if 'lactobacillus' in col.lower()]\n",
    "data['LactoSum'] = data[lacto_columns].sum(axis=1)\n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0).astype(int) # astype(int) converts True/False to 1/0 \n",
    "# drop the LactoSum column\n",
    "data = data.drop(columns=['LactoSum'])\n",
    "\n",
    "\n",
    "\n",
    "# ------------- PARTITION TEMPORARY DATA (BEFORE THRESHOLDING because thresholding should be done using only the training data)\n",
    "X = data.drop(columns=['sample_id', 'Lacto_Binary'])\n",
    "y = data['Lacto_Binary']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# ------ FIGURING OUT WHICH COLS ARE BWLOW THRESHOLD ---------- # \n",
    "species_columns = [col for col in X_train.columns if '|' in col]\n",
    "# calculate sum for each column\n",
    "columns_sums = X_train[species_columns].sum()\n",
    "# calculate total sum across all columns and rows\n",
    "total_sum = columns_sums.sum()\n",
    "# calculate percentage for each column\n",
    "columns_percentages = (columns_sums / total_sum) * 100\n",
    "# find columns where percentage is lower than 0.1% \n",
    "filtered_columns = columns_percentages[columns_percentages > 0.1]\n",
    "\n",
    "# filter X_train to keep only columns with percentage > 0.1%\n",
    "X_train_filtered = X_train[filtered_columns.index]\n",
    "\n",
    "# Remove filtered columns from X_val and X_test\n",
    "X_val_filtered = X_val[filtered_columns.index]\n",
    "X_test_filtered = X_test[filtered_columns.index]\n",
    "\n",
    "\n",
    "# ------- Perform PCA on the thresholded data ----- # \n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_filtered)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Project the validation and test sets onto the PCA space generated from the training data\n",
    "X_val_pca = pca.transform(X_val_filtered)\n",
    "X_test_pca = pca.transform(X_test_filtered)\n",
    "\n",
    "# ------ SAVE THE TRHESHOLDED DATA ------ # \n",
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test}\n",
    "with open('../../data/reduced_0_1_PCA/dataset_distribution.txt', 'w') as file:\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "for x_pca, y, name in zip([X_train_pca, X_val_pca, X_test_pca], [y_train, y_val, y_test], ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x_pca, y.values.reshape(-1, 1)])\n",
    "    column_names = [f'PC{i+1}' for i in range(x_pca.shape[1])] + ['Lacto_Binary']\n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/reduced_0_1_PCA/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Thresholded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The components explain [0.24562669 0.09070127 0.0926026  0.05162535 0.03997536 0.03354777\n",
      " 0.03274373 0.02753813 0.02651421 0.0256347 ] variance\n",
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parents[1]\n",
    "data_path = root / \"data\" / \"raw\" / \"healthy_subset_df.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "lacto_columns = [col for col in data.columns if 'lactobacillus' in col.lower()]\n",
    "data['LactoSum'] = data[lacto_columns].sum(axis=1)\n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0).astype(int) # astype(int) converts True/False to 1/0 \n",
    "# drop the LactoSum column\n",
    "data = data.drop(columns=['LactoSum'])\n",
    "\n",
    "\n",
    "\n",
    "# ------------- PARTITION TEMPORARY DATA (BEFORE THRESHOLDING because thresholding should be done using only the training data)\n",
    "X = data.drop(columns=['sample_id','Lacto_Binary'])\n",
    "y = data['Lacto_Binary']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)\n",
    "\n",
    "\n",
    "# ------ FIGURING OUT WHICH COLS ARE BWLOW THRESHOLD ---------- # \n",
    "species_columns = [col for col in X_train.columns if '|' in col]\n",
    "# calculate sum for each column\n",
    "columns_sums = X_train[species_columns].sum()\n",
    "# calculate total sum across all columns and rows\n",
    "total_sum = columns_sums.sum()\n",
    "# calculate percentage for each column\n",
    "columns_percentages = (columns_sums / total_sum) * 100\n",
    "# find columns where percentage is lower than 0.1% \n",
    "filtered_columns = columns_percentages[columns_percentages > 0.1]\n",
    "\n",
    "# filter X_train to keep only columns with percentage > 0.1%\n",
    "X_train_filtered = X_train[filtered_columns.index]\n",
    "\n",
    "# Remove filtered columns from X_val and X_test\n",
    "X_val_filtered = X_val[filtered_columns.index]\n",
    "X_test_filtered = X_test[filtered_columns.index]\n",
    "\n",
    "# ------- Perform SVD on the thresholded data ----- # \n",
    "n_components = 10  # Fixed number of components to match PCA\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_train_svd = svd.fit_transform(X_train_filtered)\n",
    "\n",
    "# Project the validation and test sets onto the SVD space generated from the training data\n",
    "X_val_svd = svd.transform(X_val_filtered)\n",
    "X_test_svd = svd.transform(X_test_filtered)\n",
    "\n",
    "# Print the explained variance ratio for consistency check\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "print(f\"The components explain {explained_variance_ratio} variance\")\n",
    "\n",
    "# ------ SAVE THE TRHESHOLDED DATA ------ # \n",
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test}\n",
    "with open('../../data/reduced_0_1_SVD/dataset_distribution.txt', 'w') as file:\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "for x_pca, y, name in zip([X_train_svd, X_val_svd, X_test_svd], [y_train, y_val, y_test], ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x_pca, y.values.reshape(-1, 1)])\n",
    "    column_names = [f'PC{i+1}' for i in range(x_pca.shape[1])] + ['Lacto_Binary']\n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/reduced_0_1_SVD/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Not-thresholded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "root = Path.cwd().parents[1]\n",
    "data_path = root / \"data\" / \"raw\" / \"healthy_subset_df.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "lacto_columns = [col for col in data.columns if 'lactobacillus' in col.lower()]\n",
    "data['LactoSum'] = data[lacto_columns].sum(axis=1)\n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0).astype(int) # astype(int) converts True/False to 1/0 \n",
    "# drop the LactoSum column\n",
    "data = data.drop(columns=['LactoSum'])\n",
    "\n",
    "\n",
    "# ------------- PARTITION TEMPORARY DATA (BEFORE THRESHOLDING)\n",
    "X = data.drop(columns=['Lacto_Binary'])\n",
    "y = data['Lacto_Binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# ------ SAVE THE TRHESHOLDED DATA ------ # \n",
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test} # create a dictionary with the datasets\n",
    "\n",
    "# open a file to write\n",
    "with open('../../data/non_reduced/dataset_distribution.txt', 'w') as file:\n",
    "    # print table headers and write to file\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    # loop through each dataset, print proportions and total counts, and write to file\n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)  # total number of observation\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "\n",
    "for x, y, name in zip([X_train, X_val, X_test], \n",
    "                      [y_train, y_val, y_test], \n",
    "                      ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.values.reshape(-1, 1)]) # combine X and y for each dataset \n",
    "    column_names = list(X.columns) + ['Lacto_Binary'] # construct list of column names, add Lacto_Binary to make sure it is the last column \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/non_reduced/{name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD SHITSHOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prepare baseline demographic data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lacto_Binary\n",
      "sample_id\n"
     ]
    }
   ],
   "source": [
    "# print the last column of the data frame to make sure that the Lacto_Binary column has been added\n",
    "print(demographic_data.columns[-1])\n",
    "# print first column to check it is the sampleID\n",
    "print(demographic_data.columns[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data.to_csv('../../data/baseline_demographic/healthy_subset_df_with_meta_lactobinary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the subset of demographic variables to use in baseline model\n",
    "selected_variables = [\n",
    "    'age', 'gender', 'country', 'BMI', 'diet', 'smoker', 'alcohol'\n",
    "] \n",
    "\n",
    "# split data into features (X) and target (y)\n",
    "X = demographic_data[selected_variables]\n",
    "y = demographic_data['Lacto_Binary']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.47 | 0.53 |     826\n",
      "Test set        | 0.50 | 0.50 |     826\n"
     ]
    }
   ],
   "source": [
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test} # create a dictionary with the datasets\n",
    "\n",
    "# open a file to write\n",
    "with open('../../data/baseline_demographic/dataset_distribution.txt', 'w') as file:\n",
    "    # print table headers and write to file\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    # loop through each dataset, print proportions and total counts, and write to file\n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)  # total number of observation\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y, name in zip([X_train, X_val, X_test], \n",
    "                      [y_train, y_val, y_test], \n",
    "                      ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.values.reshape(-1, 1)]) # combine X and y for each dataset \n",
    "    column_names = list(X.columns) + ['Lacto_Binary'] # construct list of column names, add Lacto_Binary to make sure it is the last column \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/baseline_demographic/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the non-reduced data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (5502, 1479)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3399 samples that have a LactoSum value of above 0.0\n",
      "There are 2726 samples that have a LactoSum value of above 0.01\n",
      "There are 1366 samples that have a LactoSum value of above 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Lacto_Binary distribution:\n",
      "Lacto_Binary\n",
      "0    2776\n",
      "1    2726\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "data_path = root / \"data\" / \"raw\" / \"healthy_subset_df.csv\"\n",
    "data = pd.read_csv(data_path, index_col=0)\n",
    "\n",
    "# check shape of data\n",
    "print(f\"Initial data shape: {data.shape}\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# extract indices for columns that include the word 'lactobacillus' (looking at lower-cased col names)\n",
    "lacto_index = [i for i, s in enumerate(data.columns) if 'lactobacillus' in s.lower()]\n",
    "\n",
    "# for each row, calculate the sum of the Lactobacillus species and append value to new column 'LactoSum'\n",
    "data['LactoSum'] = data.iloc[:, lacto_index].sum(axis=1) \n",
    "\n",
    "# count the number of rows that have a LactoSum value of above 0.01 and 0.0, respectively\n",
    "print(f\"There are {data[data['LactoSum'] > 0.0].shape[0]} samples that have a LactoSum value of above 0.0\")\n",
    "print(f\"There are {data[data['LactoSum'] > 0.01].shape[0]} samples that have a LactoSum value of above 0.01\")\n",
    "print(f\"There are {data[data['LactoSum'] > 0.1].shape[0]} samples that have a LactoSum value of above 0.1\")\n",
    "\n",
    "print(\"----------------------------------------------------------------------------------------------------\")\n",
    "# create binary column based on whether LactoSum is above 0.01 or not \n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0)\n",
    "\n",
    "# remove specific columns\n",
    "data.drop(data.columns[lacto_index], axis=1, inplace=True) \n",
    "data.drop('LactoSum', axis=1, inplace=True)\n",
    "\n",
    "# print count of Lacto_binary values\n",
    "print(f\"Lacto_Binary distribution:\\n{data['Lacto_Binary'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../../data/non_reduced/healthy_subset_df_with_lactobinary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lacto_Binary\n",
      "sample_id\n"
     ]
    }
   ],
   "source": [
    "# print the last column of the data frame to make sure that the Lacto_Binary column has been added\n",
    "print(data.columns[-1])\n",
    "# print first column to check it is the sampleID\n",
    "print(data.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:-1].values # make X all columns except the last first column (sample_id) and the last column (Lacto_Binary)\n",
    "y = data.iloc[:,-1].values # make y the last column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15, # 15% of data is reserved for testing\n",
    "                                                    random_state=42)\n",
    "\n",
    "# further splitting the training set into a training and a validation set (15%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, # X_train is further split into X_train and X_val\n",
    "                                                  y_train, # y_train is further split into y_train and y_val\n",
    "                                                  test_size=X_test.shape[0] / X_train.shape[0],  # 15% of the training set is reserved for validation\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test} # create a dictionary with the datasets\n",
    "\n",
    "# open a file to write\n",
    "with open('../../data/non_reduced/dataset_distribution.txt', 'w') as file:\n",
    "    # print table headers and write to file\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    # loop through each dataset, print proportions and total counts, and write to file\n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)  # total number of observation\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data splits\n",
    "for x, y, name in zip([X_train, X_val, X_test], \n",
    "                      [y_train, y_val, y_test], \n",
    "                      ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.reshape(-1, 1)]) # combine X and y for each dataset \n",
    "    column_names = list(data.drop(columns=['sample_id', 'Lacto_Binary']).columns) + ['Lacto_Binary'] # construct list of column names, add Lacto_Binary to make sure it is the last column \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/non_reduced/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the reduced data for analysis (treshold at 0.1%)\n",
    "As the dataset is high in dimensionality and sparsity, I take the following step to minimise noise and redundancy. This is an attempt to focus the analyses on features that are relevant. \n",
    "\n",
    "First, we calculate the relative abundance of each species across all samples. So for each species, we sum the relative abundance values across all observations, which gives a total abundance value for each bacterial species across all samples. Then we sum these total abundance values across all features to determine the dataset's total bacterial abundance. \n",
    "\n",
    "Then I set a threshold at 0.1%. So I only keep the species that make up 0.1% of the DNA mass across also samples.  Features (species) contributing less than this treshold are considered less informative/relevant for subsequent analyses and are thus candidates for removal. So each feature's total abundance is compared against the threshold. Features falling below this threshold are removed unless they are specifically associated with the Lactobacillus bacteria (as these are critical for the outcome variable of interest and are indexed in lacto_indx - but the lacto_indx cols wont be used as predictors themselves, but they'll be used to create the outcome variable).\n",
    "\n",
    "As each sample's feature values are given as proportions of the total bacterial abundance in that sample, summing these values across all samples for each feature essentially estimates the feature's \"average\" presence or dominance across the dataset. So a high sum indicates a greater overall presence across the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Across-samples threshold: Make a reduced dataset, thresholding species at 0.01% of total DNA mass across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lacto_Binary distribution:\n",
      "Lacto_Binary\n",
      "0    2776\n",
      "1    2726\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('../../data/raw/healthy_subset_df.csv', index_col=0)\n",
    "\n",
    "# extract indices for columns that include the word 'lactobacillus' (case-insensitive)\n",
    "lacto_index = [i for i, s in enumerate(data.columns) if 'lactobacillus' in s.lower()]\n",
    "\n",
    "# calculate the sum of Lactobacillus species for each row and append it to a new column 'LactoSum'\n",
    "data['LactoSum'] = data.iloc[:, lacto_index].sum(axis=1)\n",
    "\n",
    "# create a binary column based on whether LactoSum is above 0.01\n",
    "data['Lacto_Binary'] = np.where(data['LactoSum'] > 0.01, 1, 0)\n",
    "\n",
    "# remove the Lactobacillus columns and 'LactoSum' column\n",
    "data.drop(data.columns[lacto_index], axis=1, inplace=True)\n",
    "data.drop('LactoSum', axis=1, inplace=True)\n",
    "\n",
    "# calculate the sum of each column (excluding 'Lacto_Binary' and 'sample_id' if it exists)\n",
    "columns_to_exclude = ['Lacto_Binary']\n",
    "if 'sample_id' in data.columns:\n",
    "    columns_to_exclude.append('sample_id')\n",
    "\n",
    "column_sums = data.drop(columns=columns_to_exclude).sum()\n",
    "\n",
    "# calculate the percentage of each column sum out of the total sum\n",
    "total_sum = column_sums.sum()\n",
    "species_abundance = (column_sums / total_sum) * 100\n",
    "\n",
    "# define threshold for species abundance and identify columns to keep\n",
    "species_threshold = 0.01\n",
    "cols_to_keep = species_abundance[species_abundance > species_threshold].index.tolist()\n",
    "\n",
    "data_reduced = data[cols_to_keep + ['Lacto_Binary']]\n",
    "\n",
    "\n",
    "print(f\"Lacto_Binary distribution:\\n{data_reduced['Lacto_Binary'].value_counts()}\")\n",
    "\n",
    "# save the reduced DataFrame to a CSV file\n",
    "data_reduced.to_csv('../../data/reduced_0_1/healthy_subset_df_thresholded_01_with_lactobinary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data\n",
    "X = data_reduced.drop(columns=['Lacto_Binary']).values\n",
    "y = data_reduced['Lacto_Binary'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Further split the training set into a training and a validation set (15% of the training set for validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=X_test.shape[0] / X_train.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         |    1 |    0 |    Rows\n",
      "Training set    | 0.50 | 0.50 |    3850\n",
      "Validation set  | 0.51 | 0.49 |     826\n",
      "Test set        | 0.48 | 0.52 |     826\n"
     ]
    }
   ],
   "source": [
    "datasets = {'Training set': y_train, 'Validation set': y_val, 'Test set': y_test}\n",
    "\n",
    "# open a file to write\n",
    "with open('../../data/reduced_0_1/dataset_distribution.txt', 'w') as file:\n",
    "    header = \"{:<15} | {:>4} | {:>4} | {:>7}\".format(\"Dataset\", \"1\", \"0\", \"Rows\")\n",
    "    print(header)\n",
    "    file.write(header + '\\n')\n",
    "    \n",
    "    for name, dataset in datasets.items():\n",
    "        proportion = pd.Series(dataset).value_counts(normalize=True)\n",
    "        total = len(dataset)\n",
    "        proportion_1 = proportion.get(1, 0)\n",
    "        proportion_0 = proportion.get(0, 0)\n",
    "        line = \"{:<15} | {:.2f} | {:.2f} | {:>7}\".format(name, proportion_1, proportion_0, total)\n",
    "        print(line)\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train is (3850, 278), number of columns is 278\n",
      "Shape of val is (826, 278), number of columns is 278\n",
      "Shape of test is (826, 278), number of columns is 278\n"
     ]
    }
   ],
   "source": [
    "# save the data splits\n",
    "for x, y, name in zip([X_train, X_val, X_test], [y_train, y_val, y_test], ['train', 'val', 'test']):\n",
    "    combined_data = np.hstack([x, y.reshape(-1, 1)])\n",
    "    \n",
    "    # ensure correct column names\n",
    "    column_names = list(data_reduced.drop(columns=['Lacto_Binary']).columns) + ['Lacto_Binary']\n",
    "    \n",
    "    # debug prints\n",
    "    print(f\"Shape of {name} is {combined_data.shape}, number of columns is {combined_data.shape[1]}\")\n",
    "    \n",
    "    df = pd.DataFrame(combined_data, columns=column_names)\n",
    "    df.to_csv(f'../../data/reduced_0_1/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on thresholded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/EmmaRisgaardOlsen#9993/microbiome-ML/data/reduced_0_01/healthy_subset_df_threshold_0_01_Xy.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# file to reduced data including the Lacto_Binary variable\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/work/EmmaRisgaardOlsen#9993/microbiome-ML/data/reduced_0_01/healthy_subset_df_threshold_0_01_Xy.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# define X as all columns except Lacto_Binary\u001b[39;00m\n",
      "File \u001b[0;32m/work/EmmaRisgaardOlsen#9993/microbiome-ML/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/EmmaRisgaardOlsen#9993/microbiome-ML/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/work/EmmaRisgaardOlsen#9993/microbiome-ML/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/EmmaRisgaardOlsen#9993/microbiome-ML/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/work/EmmaRisgaardOlsen#9993/microbiome-ML/venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/EmmaRisgaardOlsen#9993/microbiome-ML/data/reduced_0_01/healthy_subset_df_threshold_0_01_Xy.csv'"
     ]
    }
   ],
   "source": [
    "# file to reduced data including the Lacto_Binary variable\n",
    "file = '.../../data/reduced_0_01/healthy_subset_df_threshold_0_01_Xy.csv'\n",
    "df = pd.read_csv(file)\n",
    "df\n",
    "\n",
    "# define X as all columns except Lacto_Binary\n",
    "X = df.drop('Lacto_Binary', axis=1)\n",
    "y = df['Lacto_Binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ APPROACH 0 ----------- # https://www.kaggle.com/code/ryanholbrook/principal-component-analysis \n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# get cumulative variance explained by the components\n",
    "cumvar = np.cumsum(X_pca.explained_variance_ratio_)\n",
    "#Plotting cumulative variance\n",
    "plt.plot(cumvar)\n",
    "plt.title('Cumulative variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Variance explained')\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose the matrix of loadings\n",
    "    columns=component_names,  # so the columns are the principal components\n",
    "    index=X.columns,  # and the rows are the original features\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at explained variance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "plot_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ APPROACH 1 ----------- #\n",
    "\n",
    "# Z-score the features - OBS I don't do it as vals are already standardized between 0 and 1 given that they are relative abundance\n",
    "#scaler = StandardScaler() \n",
    "#scaler.fit(X)\n",
    "#X = scaler.transform(X)\n",
    "\n",
    "# The PCA model\n",
    "pca = PCA(n_components=2) # estimate only 2 PCs\n",
    "X_new = pca.fit_transform(X) # project the original data into the PCA spacefig, axes = plt.subplots(1,2)\n",
    "axes[0].scatter(X[:,0], X[:,1], c=y)\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "axes[0].set_title('Before PCA')\n",
    "axes[1].scatter(X_new[:,0], X_new[:,1], c=y)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('After PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
